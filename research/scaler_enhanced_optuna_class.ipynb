{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import yaml\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import Any, Dict, Tuple\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, StandardScaler, OrdinalEncoder, PowerTransformer, RobustScaler, MinMaxScaler,\n",
    "    FunctionTransformer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import  accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "from utils_machine_learning import rename_columns_to_snake_case\n",
    "\n",
    "from custom_transformers import (\n",
    "    DropRedundantColumns,\n",
    "    CreateNewFeature,\n",
    "\n",
    "    LogTransformer,\n",
    "    PowerTransformerWrapper,\n",
    "    OutlierTransformer,\n",
    "    OutlierDetector,\n",
    "    ReplaceValueTransformer,\n",
    "    OutlierHandler,\n",
    "\n",
    ")\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import joblib\n",
    "from typing import Union\n",
    "\n",
    "from imblearn.over_sampling import (\n",
    "    RandomOverSampler,\n",
    "    ADASYN,\n",
    "    \n",
    ")\n",
    "from imblearn.under_sampling import (\n",
    "    RandomUnderSampler,\n",
    "    NearMiss,\n",
    ")\n",
    "from imblearn.combine import (\n",
    "    SMOTEENN,\n",
    "    SMOTETomek\n",
    ")\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from the CSV file and return it as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset loaded from the CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = 'https://github.com/donadviser/datasets/raw/master/data-don/auto_insurance_claim_fraud.csv'\n",
    "    data = pd.read_csv(data_path, sep=\",\")\n",
    "    return (data\n",
    "            .pipe(rename_columns_to_snake_case)\n",
    "            #.dropna()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_bind_date</th>\n",
       "      <th>policy_state</th>\n",
       "      <th>policy_csl</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>...</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>police_report_available</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>property_claim</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>auto_make</th>\n",
       "      <th>auto_model</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>fraud_reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>48</td>\n",
       "      <td>521585</td>\n",
       "      <td>2014-10-17</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1406.91</td>\n",
       "      <td>0</td>\n",
       "      <td>466132</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>71610</td>\n",
       "      <td>6510</td>\n",
       "      <td>13020</td>\n",
       "      <td>52080</td>\n",
       "      <td>Saab</td>\n",
       "      <td>92x</td>\n",
       "      <td>2004</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "      <td>42</td>\n",
       "      <td>342868</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>IN</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1197.22</td>\n",
       "      <td>5000000</td>\n",
       "      <td>468176</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>5070</td>\n",
       "      <td>780</td>\n",
       "      <td>780</td>\n",
       "      <td>3510</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>E400</td>\n",
       "      <td>2007</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134</td>\n",
       "      <td>29</td>\n",
       "      <td>687698</td>\n",
       "      <td>2000-09-06</td>\n",
       "      <td>OH</td>\n",
       "      <td>100/300</td>\n",
       "      <td>2000</td>\n",
       "      <td>1413.14</td>\n",
       "      <td>5000000</td>\n",
       "      <td>430632</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>NO</td>\n",
       "      <td>34650</td>\n",
       "      <td>7700</td>\n",
       "      <td>3850</td>\n",
       "      <td>23100</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>RAM</td>\n",
       "      <td>2007</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>41</td>\n",
       "      <td>227811</td>\n",
       "      <td>1990-05-25</td>\n",
       "      <td>IL</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1415.74</td>\n",
       "      <td>6000000</td>\n",
       "      <td>608117</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "      <td>63400</td>\n",
       "      <td>6340</td>\n",
       "      <td>6340</td>\n",
       "      <td>50720</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Tahoe</td>\n",
       "      <td>2014</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "      <td>44</td>\n",
       "      <td>367455</td>\n",
       "      <td>2014-06-06</td>\n",
       "      <td>IL</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1583.91</td>\n",
       "      <td>6000000</td>\n",
       "      <td>610706</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>6500</td>\n",
       "      <td>1300</td>\n",
       "      <td>650</td>\n",
       "      <td>4550</td>\n",
       "      <td>Accura</td>\n",
       "      <td>RSX</td>\n",
       "      <td>2009</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   months_as_customer  age  policy_number policy_bind_date policy_state  \\\n",
       "0                 328   48         521585       2014-10-17           OH   \n",
       "1                 228   42         342868       2006-06-27           IN   \n",
       "2                 134   29         687698       2000-09-06           OH   \n",
       "3                 256   41         227811       1990-05-25           IL   \n",
       "4                 228   44         367455       2014-06-06           IL   \n",
       "\n",
       "  policy_csl  policy_deductable  policy_annual_premium  umbrella_limit  \\\n",
       "0    250/500               1000                1406.91               0   \n",
       "1    250/500               2000                1197.22         5000000   \n",
       "2    100/300               2000                1413.14         5000000   \n",
       "3    250/500               2000                1415.74         6000000   \n",
       "4   500/1000               1000                1583.91         6000000   \n",
       "\n",
       "   insured_zip  ... witnesses police_report_available total_claim_amount  \\\n",
       "0       466132  ...         2                     YES              71610   \n",
       "1       468176  ...         0                       ?               5070   \n",
       "2       430632  ...         3                      NO              34650   \n",
       "3       608117  ...         2                      NO              63400   \n",
       "4       610706  ...         1                      NO               6500   \n",
       "\n",
       "  injury_claim property_claim  vehicle_claim  auto_make auto_model auto_year  \\\n",
       "0         6510          13020          52080       Saab        92x      2004   \n",
       "1          780            780           3510   Mercedes       E400      2007   \n",
       "2         7700           3850          23100      Dodge        RAM      2007   \n",
       "3         6340           6340          50720  Chevrolet      Tahoe      2014   \n",
       "4         1300            650           4550     Accura        RSX      2009   \n",
       "\n",
       "  fraud_reported  \n",
       "0              Y  \n",
       "1              Y  \n",
       "2              N  \n",
       "3              Y  \n",
       "4              N  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = load_dataset()\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_features = ['policy_state', 'collision_type', 'property_damage', 'police_report_available', \n",
    "                  'insured_sex', 'insured_education_level', 'insured_relationship', 'incident_type', \n",
    "                  'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', \n",
    "                  'policy_deductable', 'number_of_vehicles_involved', 'bodily_injuries', 'witnesses', \n",
    "                  'incident_period_of_day']\n",
    "\n",
    "numerical_features = ['months_as_customer',  'age', 'policy_annual_premium', 'injury_claim', \n",
    "                      'property_claim', 'vehicle_claim', 'vehicle_age','total_claim_amount']\n",
    "\n",
    "ordinal_features = ['insured_occupation', 'insured_hobbies', 'auto_make']\n",
    "\n",
    "transform_features = ['umbrella_limit', 'capital_gains', 'capital_loss']\n",
    "\n",
    "drop_columns = ['policy_number','policy_bind_date','policy_csl', 'insured_zip','incident_date',\n",
    "                'incident_location','auto_model','auto_year', 'incident_hour_of_the_day',\n",
    "                ]\n",
    "\n",
    "bins_hour = [0, 6, 11, 16, 21, 24]  # Time bins for different periods of the day\n",
    "names_period = [\"early_morning\", \"morning\", \"afternoon\", \"evening\", \"night\"] \n",
    "\n",
    "target_col = 'fraud_reported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns= ['policy_state', 'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', 'property_damage', 'police_report_available', 'auto_make', 'policy_deductable', 'number_of_vehicles_involved', 'bodily_injuries', 'witnesses', 'fraud_reported', 'incident_hour_of_the_day']\n",
    "numerical_columns =['months_as_customer', 'age', 'policy_annual_premium', 'umbrella_limit', 'capital_gains', 'capital_loss', 'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min - Max values for numerical Column: months_as_customer\n",
      "0 - 479\n",
      "----------------------------------------\n",
      "\n",
      "Min - Max values for numerical Column: age\n",
      "19 - 64\n",
      "----------------------------------------\n",
      "\n",
      "Min - Max values for numerical Column: policy_annual_premium\n",
      "433.33 - 2047.59\n",
      "----------------------------------------\n",
      "\n",
      "Min - Max values for numerical Column: umbrella_limit\n",
      "-1000000 - 10000000\n",
      "----------------------------------------\n",
      "\n",
      "Min - Max values for numerical Column: capital_gains\n",
      "0 - 100500\n",
      "----------------------------------------\n",
      "\n",
      "Min - Max values for numerical Column: capital_loss\n",
      "-111100 - 0\n",
      "----------------------------------------\n",
      "\n",
      "Min - Max values for numerical Column: total_claim_amount\n",
      "100 - 114920\n",
      "----------------------------------------\n",
      "\n",
      "Min - Max values for numerical Column: injury_claim\n",
      "0 - 21450\n",
      "----------------------------------------\n",
      "\n",
      "Min - Max values for numerical Column: property_claim\n",
      "0 - 23670\n",
      "----------------------------------------\n",
      "\n",
      "Min - Max values for numerical Column: vehicle_claim\n",
      "70 - 79560\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in numerical_columns:\n",
    "    print(f'Min - Max values for numerical Column: {item}')\n",
    "    print(f\"{data_raw[item].min()} - {data_raw[item].max()}\")\n",
    "    print('----------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset is loaded in a pandas dataframe\n",
    "X, y = data_raw.drop(columns=[target_col]), data_raw[target_col]\n",
    "\n",
    "# Splitting the dataset\n",
    "y = y.map({'Y': 1, 'N': 0})  # Map target labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML configuration file\n",
    "def load_yaml_config(config_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the YAML configuration file containing model and hyperparameter definitions.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the YAML configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The loaded configuration as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    \"\"\"\n",
    "    HyperparameterTuner to return hyperparameters for each classifier.\n",
    "    \"\"\"\n",
    "    def get_params(self, trial: optuna.Trial, classifier_name: str):\n",
    "        if classifier_name == \"RandomForest\":\n",
    "            return {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "            }\n",
    "        elif classifier_name == \"DecisionTree\":\n",
    "            return {\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "            }\n",
    "        elif classifier_name == \"LGBM\":\n",
    "            return {\n",
    "                \"objective\": \"binary\",\n",
    "                \"metric\": \"binary_logloss\",\n",
    "                \"verbosity\": -1,\n",
    "                \"boosting_type\": \"gbdt\",\n",
    "                \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "                \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "                \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "                \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            }\n",
    "        elif classifier_name == \"XGBoost\":\n",
    "            return {\n",
    "                \"verbosity\": 0,\n",
    "                \"objective\": \"binary:logistic\",\n",
    "                \"eval_metric\": \"auc\",\n",
    "                \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "                \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "                \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "            }\n",
    "        elif classifier_name == \"CatBoost\":\n",
    "            return {\n",
    "                \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "                \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "                \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "                \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "                \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n",
    "            }\n",
    "        elif classifier_name == \"LogisticRegression\":\n",
    "            # Basic hyperparameters\n",
    "            params = {\n",
    "                \"solver\": trial.suggest_categorical('solver', ['newton-cholesky', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "                \"max_iter\": trial.suggest_int('max_iter', 10000, 50000),  # Increased max_iter to allow for better convergence\n",
    "            }\n",
    "\n",
    "            # Suggest penalty from a unified set\n",
    "            all_penalties = ['l1', 'l2', 'elasticnet', None]  # Unified penalties\n",
    "            params['penalty'] = trial.suggest_categorical('penalty', all_penalties)\n",
    "\n",
    "            # Only suggest C if penalty is not None\n",
    "            if params['penalty'] is not None:\n",
    "                params[\"C\"] = trial.suggest_float('C', 1e-10, 1000, log=True)\n",
    "            \n",
    "            # Only suggest l1_ratio if penalty is 'elasticnet'\n",
    "            if params['penalty'] == 'elasticnet':\n",
    "                params['l1_ratio'] = trial.suggest_float('l1_ratio', 0, 1)\n",
    "\n",
    "            # Prune invalid combinations:\n",
    "            if (\n",
    "                (params['solver'] == 'lbfgs' and params['penalty'] not in ['l2', None]) or\n",
    "                (params['solver'] == 'liblinear' and params['penalty'] not in ['l1', 'l2']) or\n",
    "                (params['solver'] == 'sag' and params['penalty'] not in ['l2', None]) or\n",
    "                (params['solver'] == 'newton-cholesky' and params['penalty'] not in ['l2', None]) or\n",
    "                (params['solver'] == 'saga' and params['penalty'] not in ['elasticnet', 'l1', 'l2', None])\n",
    "            ):\n",
    "                raise optuna.TrialPruned()  # Invalid combination of solver and penalty\n",
    "\n",
    "            return params\n",
    "\n",
    "        \n",
    "        elif classifier_name == \"GradientBoosting\":\n",
    "            return {\n",
    "                \"learning_rate\" : trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "                \"n_estimators\" : trial.suggest_int('n_estimators', 100, 1000),\n",
    "                \"max_depth\" : trial.suggest_int('max_depth', 3, 10),\n",
    "                \"min_samples_split\" : trial.suggest_int('min_samples_split', 2, 20),\n",
    "                \"min_samples_leaf\" : trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "                \"max_features\" : trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    \n",
    "            }\n",
    "        elif classifier_name == \"KNeighbors\":\n",
    "            params = {\n",
    "                \"n_neighbors\": trial.suggest_int('n_neighbors', 1, 50),\n",
    "                \"weights\": trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "                \"p\": trial.suggest_int('p', 1, 2),  # 1: Manhattan, 2: Euclidean\n",
    "                \"leaf_size\": trial.suggest_int('leaf_size', 10, 100),\n",
    "                \"metric\": trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski', 'chebyshev'])\n",
    "            }\n",
    "            return params\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid classifier name: {classifier_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFactory:\n",
    "    \"\"\"\n",
    "    A class to create model instances with additional parameters for specific classifiers.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): The name of the model to be instantiated.\n",
    "        best_params (dict): The best hyperparameters for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, best_params: dict):\n",
    "        \"\"\"\n",
    "        Initialize the ModelFactory with a model name and parameters.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The name of the model.\n",
    "            best_params (dict): Hyperparameters for the model.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.best_params = best_params\n",
    "\n",
    "    def get_model_instance(self):\n",
    "        \"\"\"\n",
    "        Creates a model instance based on the model name with additional classifier-specific parameters.\n",
    "\n",
    "        Returns:\n",
    "            A model instance with the appropriate parameters.\n",
    "        \"\"\"\n",
    "        # Dictionary of model classes\n",
    "        model_dict = {\n",
    "            \"LGBM\": LGBMClassifier,\n",
    "            \"XGBoost\": XGBClassifier,\n",
    "            \"CatBoost\": CatBoostClassifier,\n",
    "            \"RandomForest\": RandomForestClassifier,\n",
    "            \"DecisionTree\": DecisionTreeClassifier,\n",
    "            \"LogisticRegression\": LogisticRegression,\n",
    "            \"SVC\": SVC,\n",
    "            \"GradientBoosting\": GradientBoostingClassifier,\n",
    "            \"KNeighbors\": KNeighborsClassifier\n",
    "        }\n",
    "\n",
    "        # Check if the model exists in the model_dict\n",
    "        if self.model_name not in model_dict:\n",
    "            raise ValueError(f\"Model {self.model_name} is not supported.\")\n",
    "\n",
    "        # Create a model instance with specific parameters\n",
    "        if self.model_name == \"LGBM\":\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42, verbose=-1)  # Add verbose for LGBM\n",
    "        elif self.model_name == \"RandomForest\":\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42, n_jobs=-1)  # Add n_jobs for RandomForest\n",
    "        elif self.model_name == \"SVC\":\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42, probability=True)  # Add probability for SVC\n",
    "        elif self.model_name == \"CatBoost\":\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42, verbose=0)  # Suppress CatBoost verbosity\n",
    "        elif self.model_name == \"KNeighbors\":\n",
    "            return model_dict[self.model_name](**self.best_params)  # Suppress CatBoost verbosity\n",
    "        else:\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42)  # Default for other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineManager:\n",
    "    \"\"\"\n",
    "    A class that handles both building and modifying pipelines dynamically.\n",
    "    This class supports both scikit-learn's Pipeline and imbalanced-learn's Pipeline.\n",
    "\n",
    "    It allows the construction of the initial pipeline and the insertion of steps \n",
    "    at any position within the pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pipeline_type='ImbPipeline'):\n",
    "        \"\"\"\n",
    "        Initialize the PipelineManager with a specified pipeline type.\n",
    "\n",
    "        Args:\n",
    "            pipeline_type (str): The type of pipeline to use ('ImbPipeline' or 'Pipeline').\n",
    "        \"\"\"\n",
    "        if pipeline_type == 'ImbPipeline':\n",
    "            self.pipeline = ImbPipeline(steps=[])\n",
    "        elif pipeline_type == 'Pipeline':\n",
    "            self.pipeline = Pipeline(steps=[])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pipeline type. Choose 'ImbPipeline' or 'Pipeline'.\")\n",
    "\n",
    "    def add_step(self, step_name, step_object, position=None):\n",
    "        \"\"\"\n",
    "        Add a transformation step to the pipeline.\n",
    "\n",
    "        Args:\n",
    "            step_name (str): Name of the step to add.\n",
    "            step_object (object): The transformer or estimator object (e.g., scaler, classifier).\n",
    "            position (int or None): Optional; the position to insert the step.\n",
    "                                    If None, the step is appended at the end of the pipeline.\n",
    "        \"\"\"\n",
    "        if position is None:\n",
    "            self.pipeline.steps.append((step_name, step_object))\n",
    "        else:\n",
    "            self.pipeline.steps.insert(position, (step_name, step_object))\n",
    "\n",
    "    def remove_step(self, step_name):\n",
    "        \"\"\"\n",
    "        Remove a step from the pipeline by its name.\n",
    "\n",
    "        Args:\n",
    "            step_name (str): The name of the step to remove.\n",
    "        \"\"\"\n",
    "        self.pipeline.steps = [(name, step) for name, step in self.pipeline.steps if name != step_name]\n",
    "\n",
    "    def replace_step(self, step_name, new_step_object):\n",
    "        \"\"\"\n",
    "        Replace an existing step in the pipeline with a new step.\n",
    "\n",
    "        Args:\n",
    "            step_name (str): The name of the step to replace.\n",
    "            new_step_object (object): The new transformer or estimator object.\n",
    "        \"\"\"\n",
    "        for i, (name, step) in enumerate(self.pipeline.steps):\n",
    "            if name == step_name:\n",
    "                self.pipeline.steps[i] = (step_name, new_step_object)\n",
    "                break\n",
    "\n",
    "    def get_pipeline(self):\n",
    "        \"\"\"\n",
    "        Get the constructed or modified pipeline.\n",
    "\n",
    "        Returns:\n",
    "            Pipeline: The constructed or modified pipeline object.\n",
    "        \"\"\"\n",
    "        return self.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "    \"\"\"\n",
    "    A class that encapsulates the preprocessing steps for feature engineering,\n",
    "    imputation, scaling, encoding, and transformations. This can be inserted into\n",
    "    the overall pipeline before the model fitting step.\n",
    "    \"\"\"\n",
    "    def __init__(self, bins_hour, names_period, drop_columns, numerical_features,\n",
    "                 onehot_features, ordinal_features, transform_features, trial: optuna.Trial=None):\n",
    "        \"\"\"\n",
    "        Initialize the PreprocessingPipeline with necessary parameters.\n",
    "\n",
    "        Args:\n",
    "            bins_hour: Parameters for creating new features from hourly bins.\n",
    "            names_period: Period names for feature creation.\n",
    "            drop_columns: Columns to be dropped from the dataset.\n",
    "            numerical_features: List of numerical features for processing.\n",
    "            onehot_features: List of categorical features for OneHot encoding.\n",
    "            ordinal_features: List of ordinal features for Ordinal encoding.\n",
    "            transform_features: Features that require power transformation.\n",
    "        \"\"\"\n",
    "        self.bins_hour = bins_hour\n",
    "        self.names_period = names_period\n",
    "        self.drop_columns = drop_columns\n",
    "        self.numerical_features = numerical_features\n",
    "        self.onehot_features = onehot_features\n",
    "        self.ordinal_features = ordinal_features\n",
    "        self.transform_features = transform_features\n",
    "        \n",
    "    def instantiate_numerical_simple_imputer(self, trial: optuna.Trial=None, strategy: str=None, fill_value: int=-1) -> SimpleImputer:\n",
    "        if strategy is None and trial:\n",
    "            strategy = trial.suggest_categorical(\n",
    "                'numerical_strategy', ['mean', 'median', 'most_frequent']\n",
    "            )\n",
    "        #print(f\"instantiate_numerical_simple_imputer: strategy= {strategy}\")\n",
    "        return SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "\n",
    "    def instantiate_categorical_simple_imputer(self, trial: optuna.Trial=None, strategy: str=None, fill_value: str='missing') -> SimpleImputer:\n",
    "        if strategy is None and trial:\n",
    "            strategy = trial.suggest_categorical('categorical_strategy', ['most_frequent', 'constant'])\n",
    "        #print(f\"instantiate_categorical_simple_imputer: strategy= {strategy}\")\n",
    "        return SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "    \n",
    "    def instantiate_outliers(self, trial: optuna.Trial=None, strategy=None) -> Union[PowerTransformer, FunctionTransformer, OutlierDetector]:\n",
    "        \"\"\"\n",
    "        Instantiate outlier handling method: PowerTransformer, LogTransformer, or OutlierDetector.\n",
    "\n",
    "        Args:\n",
    "            trial (optuna.Trial, optional): The trial object for hyperparameter optimization.\n",
    "\n",
    "        Returns:\n",
    "            Union[PowerTransformer, FunctionTransformer, OutlierDetector]: The selected outlier handling method.\n",
    "        \"\"\"\n",
    "        # Suggest from available options\n",
    "        options = ['power_transform', 'log_transform', 'iqr_clip', 'iqr_median', 'iqr_mean']\n",
    "        if trial:\n",
    "            strategy = trial.suggest_categorical('outlier_strategy', options)\n",
    "        else:\n",
    "            strategy = strategy  # Default to first option if no trial is provided\n",
    "\n",
    "        if strategy == 'power_transform':\n",
    "            return PowerTransformer(method='yeo-johnson')\n",
    "        elif strategy == 'log_transform':\n",
    "            return LogTransformer()\n",
    "            #return FunctionTransformer(np.log1p)  # Log transformation\n",
    "        elif strategy in ['iqr_clip', 'iqr_median', 'iqr_mean']:\n",
    "            return OutlierHandler(strategy=strategy)  # Instantiate OutlierDetector\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy for outlier handling: {strategy}\")\n",
    "\n",
    "         \n",
    "    def build(self, step_name=None, trial: optuna.Trial=None, **column_transformer_strategy):\n",
    "        \"\"\"\n",
    "        Build the preprocessing pipeline with feature creation, transformation, \n",
    "        imputation, scaling, and encoding steps.\n",
    "        \n",
    "        Returns:\n",
    "            Transformer: The appropriate transformer for the given step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if step_name == \"create_new_features\":\n",
    "            return CreateNewFeature(bins_hour=self.bins_hour, names_period=self.names_period)\n",
    "        \n",
    "        if step_name == \"replace_class\":\n",
    "            return ReplaceValueTransformer(old_value=\"?\", new_value=np.nan)\n",
    "        \n",
    "        if step_name == \"drop_cols\":\n",
    "            return DropRedundantColumns(redundant_cols=self.drop_columns)\n",
    "        \n",
    "        if step_name == 'column_transformer':\n",
    "            \n",
    "            numerical_strategy = column_transformer_strategy.get('numerical_strategy', None)\n",
    "            categorical_strategy = column_transformer_strategy.get('categorical_strategy',None)\n",
    "            outlier_strategy = column_transformer_strategy.get('outlier_strategy', None)\n",
    "        \n",
    "            \n",
    "            return ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('categorical', Pipeline([\n",
    "                        ('imputer', self.instantiate_categorical_simple_imputer(trial=trial, strategy=categorical_strategy)),   \n",
    "                        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))\n",
    "                    ]), self.onehot_features),\n",
    "                    \n",
    "                    ('numerical', Pipeline([\n",
    "                        ('imputer', self.instantiate_numerical_simple_imputer(trial=trial, strategy=numerical_strategy)),\n",
    "                        #('scaler', StandardScaler())  # Add scaler if needed\n",
    "                    ]), self.numerical_features),\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    ('ordinal', Pipeline([\n",
    "                        ('imputer', self.instantiate_categorical_simple_imputer(trial=trial, strategy=categorical_strategy)),\n",
    "                        ('ordinal', OrdinalEncoder())\n",
    "                    ]), self.ordinal_features),\n",
    "                    \n",
    "                    ('outlier_transform', Pipeline([\n",
    "                        ('imputer', self.instantiate_numerical_simple_imputer(trial=trial, strategy=numerical_strategy)),\n",
    "                        ('outlier_transformer', self.instantiate_outliers(trial=trial, strategy=outlier_strategy))  # Update this line\n",
    "                    ]), self.transform_features),\n",
    "                ],\n",
    "                remainder='passthrough'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResamplerSelector:\n",
    "    \"\"\"\n",
    "    A class to select and return a resampling algorithm based on a given parameter or \n",
    "    from a trial suggestion if available.\n",
    "\n",
    "    Attributes:\n",
    "        trial (optuna.trial, optional): The trial object for hyperparameter optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trial=None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the ResamplerSelector with an optional trial for hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            trial (optuna.trial, optional): An optional trial object for suggesting resampling strategies.\n",
    "            random_state (int): Random seed for reproducibility. Default is 42.\n",
    "        \"\"\"\n",
    "        self.trial = trial\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_resampler(self, resampler=None):\n",
    "        \"\"\"\n",
    "        Return the resampling algorithm based on the provided `resampler` parameter.\n",
    "        If `resampler` is not given, it is suggested from the trial.\n",
    "\n",
    "        Args:\n",
    "            resampler (str, optional): The resampling method ('RandomOverSampler', 'ADASYN', etc.). \n",
    "                                       If not provided, it will be suggested from the trial (if available).\n",
    "\n",
    "        Returns:\n",
    "            resampler_obj (object): The resampling instance based on the selected method.\n",
    "        \"\"\"\n",
    "        if resampler is None and self.trial:\n",
    "            resampler = self.trial.suggest_categorical(\n",
    "                'resampler', ['RandomOverSampler', 'SMOTEENN', 'SMOTETomek']\n",
    "            )\n",
    "            #['RandomOverSampler', 'ADASYN', 'RandomUnderSampler', 'NearMiss', 'SMOTEENN', 'SMOTETomek']\n",
    "\n",
    "        if resampler == 'RandomOverSampler':\n",
    "            return RandomOverSampler(random_state=self.random_state)\n",
    "        elif resampler == 'ADASYN':\n",
    "            return ADASYN(random_state=self.random_state)\n",
    "        elif resampler == 'RandomUnderSampler':\n",
    "            return RandomUnderSampler(random_state=self.random_state)\n",
    "        elif resampler == 'NearMiss':\n",
    "            return NearMiss()\n",
    "        elif resampler == 'SMOTEENN':\n",
    "            return SMOTEENN(random_state=self.random_state, sampling_strategy='minority' )\n",
    "        elif resampler == 'SMOTETomek':\n",
    "            return SMOTETomek(random_state=self.random_state)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown resampler: {resampler}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalerSelector:\n",
    "    \"\"\"\n",
    "    A class to select and return a scaling algorithm based on a given parameter or \n",
    "    from a trial suggestion if available.\n",
    "\n",
    "    Attributes:\n",
    "        trial (optuna.trial, optional): The trial object for hyperparameter optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trial=None):\n",
    "        \"\"\"\n",
    "        Initialize the ScalerSelector with an optional trial for hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            trial (optuna.trial, optional): An optional trial object for suggesting resampling strategies.\n",
    "        \"\"\"\n",
    "        self.trial = trial\n",
    "\n",
    "    def get_scaler(self, scaler_name=None):\n",
    "        \"\"\"\n",
    "        Return the scaling algorithm based on the provided `scaler_name` parameter.\n",
    "        If `scaler_name` is not given, it is suggested from the trial.\n",
    "\n",
    "        Args:\n",
    "            scaler_name (str, optional): The scalring method ('MinMaxScaler', 'StandardScaler', etc.). \n",
    "                                       If not provided, it will be suggested from the trial (if available).\n",
    "\n",
    "        Returns:\n",
    "            rscaler_obj (object): The scaling instance based on the selected method.\n",
    "        \"\"\" \n",
    "         \n",
    "        # -- Instantiate scaler (skip scaler for CatBoost as it handles categorical features internally)\n",
    "        if scaler_name is None and self.trial:\n",
    "            scaler_name = self.trial.suggest_categorical(\"scaler\", ['minmax', 'standard', 'robust'])\n",
    "            \n",
    "        if scaler_name == \"minmax\":\n",
    "            return MinMaxScaler()\n",
    "        elif scaler_name == \"standard\":\n",
    "            return StandardScaler()\n",
    "        elif scaler_name == \"robust\":\n",
    "            return RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scaler: {scaler_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionalityReductionSelector:\n",
    "    \"\"\"\n",
    "    A class to select and return a dimensionality reduction algorithm based on a given parameter \n",
    "    or from a trial suggestion if available.\n",
    "\n",
    "    Attributes:\n",
    "        trial (optuna.trial, optional): The trial object for hyperparameter optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trial=None):\n",
    "        \"\"\"\n",
    "        Initialize the DimensionalityReductionSelector with an optional trial for hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            trial (optuna.trial, optional): An optional trial object for suggesting dimensionality reduction strategies.\n",
    "        \"\"\"\n",
    "        self.trial = trial\n",
    "\n",
    "    def get_dimensionality_reduction(self, dim_red=None, pca_n_components=5):\n",
    "        \"\"\"\n",
    "        Return the dimensionality reduction algorithm based on the provided `dim_red` parameter.\n",
    "        If `dim_red` is not given, it is suggested from the trial.\n",
    "\n",
    "        Args:\n",
    "            dim_red (str, optional): The dimensionality reduction method ('PCA' or None). If not provided,\n",
    "                                     it will be suggested from the trial (if available).\n",
    "\n",
    "        Returns:\n",
    "            dimen_red_algorithm (object or str): PCA algorithm or 'passthrough'.\n",
    "        \"\"\"\n",
    "        if dim_red is None and self.trial:\n",
    "            dim_red = self.trial.suggest_categorical(\"dim_red\", [\"PCA\", None])\n",
    "\n",
    "        if dim_red == \"PCA\":\n",
    "            if self.trial:\n",
    "                pca_n_components = self.trial.suggest_int(\"pca_n_components\", 2, 30)\n",
    "            else:\n",
    "                pca_n_components = pca_n_components  # Default value if trial is not provided\n",
    "            dimen_red_algorithm = PCA(n_components=pca_n_components)\n",
    "        else:\n",
    "            dimen_red_algorithm = 'passthrough'\n",
    "\n",
    "        return dimen_red_algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_model_and_params(classifier_name, trial, model_mode='tune', input_params=None):\n",
    "    # Got the Preprocessed Pipeline containting Data Cleaning and Column Transformation\n",
    "    \n",
    "                    \n",
    "    preprocessing_pipeline = PreprocessingPipeline(\n",
    "        bins_hour=bins_hour,\n",
    "        names_period=names_period,\n",
    "        drop_columns=drop_columns,\n",
    "        numerical_features=numerical_features,\n",
    "        onehot_features=onehot_features,\n",
    "        ordinal_features=ordinal_features,\n",
    "        transform_features=transform_features\n",
    "    )\n",
    "\n",
    "\n",
    "    #print(f\"get_pipeline_model_and_params: Starting input params: {input_params}\")\n",
    "\n",
    "    # Initialize the manager with the preferred pipeline type ('ImbPipeline' or 'Pipeline')\n",
    "    pipeline_manager = PipelineManager(pipeline_type='ImbPipeline')\n",
    "    \n",
    "    pipeline_manager.add_step('create_new_features', preprocessing_pipeline.build(step_name='create_new_features', trial=None), position=0)\n",
    "    pipeline_manager.add_step('replace_class', preprocessing_pipeline.build(step_name='replace_class', trial=None), position=1)\n",
    "    pipeline_manager.add_step('drop_cols', preprocessing_pipeline.build(step_name='drop_cols', trial=None), position=2)\n",
    "\n",
    "    \n",
    "    if model_mode == 'tune':\n",
    "        # Add transformation steps: Option 3       \n",
    "        pipeline_manager.add_step('column_transformer', preprocessing_pipeline.build(step_name='column_transformer', trial=trial), position=3)\n",
    "    else:\n",
    "        # Add transformation steps: Option 3 = Works perfectly\n",
    "        numerical_strategy = input_params.pop('numerical_strategy', 'mean')\n",
    "        categorical_strategy = input_params.pop('categorical_strategy','most_frequent')\n",
    "        outlier_strategy = input_params.pop('outlier_strategy', 'power_transform')\n",
    "        \n",
    "        print(f\"numerical_strategy: {numerical_strategy}\")\n",
    "        print(f\"categorical_strategy: {categorical_strategy}\")\n",
    "        print(f\"outlier_strategy: {outlier_strategy}\")\n",
    "        \n",
    "        \n",
    "        column_transformer_strategy ={\n",
    "            \"numerical_strategy\": numerical_strategy,\n",
    "            \"categorical_strategy\": categorical_strategy,\n",
    "            \"outlier_strategy\": outlier_strategy,\n",
    "            \"missing_values\": 'mean',\n",
    "            \"handle_unknown\": 'ignore'  # It's important to handle unknown categorical values correctly when doing feature engineering.\n",
    "        }\n",
    "        \n",
    "        pipeline_manager.add_step('column_transformer', preprocessing_pipeline.build(step_name='column_transformer', trial=None, **column_transformer_strategy), position=3)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Add the resampler step based on the provided resample name or trial suggestion\n",
    "    resample_selector = ResamplerSelector(trial=trial) \n",
    "    resampler = input_params.pop('resampler', None)   \n",
    "    resampler_obj = resample_selector.get_resampler(resampler=resampler)\n",
    "    pipeline_manager.add_step('resampler', resampler_obj, position=4)\n",
    "    \n",
    "    \n",
    "    # Add the scaler step based on the provided resample name or trial suggestion\n",
    "    scaler_selector = ScalerSelector(trial=trial)\n",
    "    scaler_name = input_params.pop('scaler', None)      \n",
    "    scaler_obj = scaler_selector.get_scaler(scaler_name=scaler_name)\n",
    "    pipeline_manager.add_step('scaler', scaler_obj, position=5)\n",
    "    \n",
    "    \n",
    "    # Add the Dimensional Reduction step based on the provided parameter or trial suggestion\n",
    "    dim_red_selector = DimensionalityReductionSelector(trial=trial)\n",
    "    dim_red = input_params.pop('dim_red', None)\n",
    "    pca_n_components = input_params.get('pca_n_components', 5)  \n",
    "    dim_red_obj = dim_red_selector.get_dimensionality_reduction(dim_red=dim_red, pca_n_components=pca_n_components)\n",
    "    pipeline_manager.add_step('dim_reduction', dim_red_obj, position=6)\n",
    "\n",
    "    # Create an instance of the ModelFactory class with best_model and best_params\n",
    "    #print(f\"Input params into the classifer:\\n{input_params}\")\n",
    "    model_factory = ModelFactory(classifier_name, input_params)\n",
    "    model_obj = model_factory.get_model_instance()\n",
    "    pipeline_manager.add_step('model', model_obj, position=7)\n",
    "    \n",
    "    pipeline = pipeline_manager.get_pipeline()\n",
    "    \n",
    "    return pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function for Optuna\n",
    "def objective(classifier_name: str, trial: optuna.Trial=None, scoring='f1') -> float:\n",
    "    \"\"\"\n",
    "    Objective function to optimize classifiers dynamically using Optuna.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object for suggesting hyperparameters.\n",
    "        classifier_name (str): Classifier to optimize.\n",
    "        scoring (str): Scoring metric for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean score from cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get hyperparameters for the classifier from HyperparameterTuner\n",
    "    hyperparameter_tuner = HyperparameterTuner()\n",
    "    params = hyperparameter_tuner.get_params(trial, classifier_name)\n",
    "    #print(\"hyperparameter parameters obtained from HyperparameterTuner class\")\n",
    "    \n",
    "    # Got the Preprocessed Pipeline containting Data Cleaning and Column Transformation\n",
    "    \n",
    "    pipeline = get_pipeline_model_and_params(classifier_name, trial, model_mode='tune', input_params=params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    score = cross_val_score(pipeline, X_train, y_train, scoring=scoring, n_jobs=-1, cv=kfold, verbose=0, error_score='raise')\n",
    "    score_training = score.mean()\n",
    "    \n",
    "    #pipeline.fit(X_train, y_train)\n",
    "    #score_testing = pipeline.score(X_test, y_test)\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        \n",
    "    tuning_test_metrics = {\n",
    "    'f1': f1,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'roc_auc': roc_auc,\n",
    "    'training_score': score_training,\n",
    "\n",
    "}\n",
    "    \n",
    "    return tuning_test_metrics[scoring]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another objective function that shares most of the `objective` function to reproduce the model with the best hyperparameters.\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def detailed_objective(classifier_name: str, trial: optuna.Trial=None, scoring='f1') -> Tuple[float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Objective function to optimize classifiers dynamically using Optuna.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object for suggesting hyperparameters.\n",
    "        classifier_name (str): Classifier to optimize.\n",
    "        scoring (str): Scoring metric for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean score from cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get hyperparameters for the classifier from HyperparameterTuner\n",
    "    hyperparameter_tuner = HyperparameterTuner()\n",
    "    params = hyperparameter_tuner.get_params(trial, classifier_name)\n",
    "    #print(\"hyperparameter parameters obtained from HyperparameterTuner class\")\n",
    "    \n",
    "    # Got the Preprocessed Pipeline containting Data Cleaning and Column Transformation\n",
    "    \n",
    "    pipeline = get_pipeline_model_and_params(classifier_name, trial, model_mode='tune', input_params=params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    score = cross_val_score(pipeline, X_train, y_train, scoring=scoring, n_jobs=-1, cv=kfold, verbose=0, error_score='raise')\n",
    "    score_training = score.mean()\n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    metrics_test = {\n",
    "        'classifier_name': classifier_name,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc,\n",
    "        f'score_training_{scoring}': score_training,\n",
    "        'best_params': trial,\n",
    "        \"classification_report\": classification_report(y_test, y_pred, output_dict=True)  # Detailed report\n",
    "        }\n",
    "    \n",
    "    # Save the variables to a file\n",
    "    # Serialise the trained pipeline\n",
    "    joblib.dump((pipeline, metrics_test), f'{classifier_name}_pipeline.pkl')\n",
    "    print(f'Serialized {classifier_name} pipeline and test metrics to {classifier_name}_pipeline.pkl')\n",
    "    \n",
    "     \n",
    "\n",
    "    return metrics_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Optuna study\n",
    "def run_optimization(config_path: str, n_trials: int = 100, scoring: str = 'f1') -> None:\n",
    "    \"\"\"\n",
    "    Run Optuna study for hyperparameter tuning and model selection.\n",
    "    \n",
    "    Args:\n",
    "        config_path (str): Path to the YAML configuration file.\n",
    "        n_trials (int): Number of trials for optimization. Defaults to 100.\n",
    "        scoring (str): Scoring metric for optimization. Defaults to 'f1'.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_model_score = 0\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    best_of_models = []\n",
    "\n",
    "    all_models = [\"RandomForest\", \"DecisionTree\", \n",
    "                  \"XGBoost\", \"LGBM\", \"GradientBoosting\", \n",
    "                  \"LogisticRegression\", \"KNeighbors\", \"CatBoost\"]\n",
    "    \n",
    "    #all_models = [\"DecisionTree\", \"RandomForest\", \"LogisticRegression\"]\n",
    "\n",
    "    for classifier_name in all_models:\n",
    "        print(f\"Optimizing model: {classifier_name} | Scoring: {scoring}\")\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=11))\n",
    "        study.optimize(lambda trial: objective(classifier_name, trial, scoring), n_trials=n_trials)\n",
    "        \n",
    "        print(f\"classifier_name: {classifier_name} | study.best_trial.value: {study.best_trial.value}\")\n",
    "\n",
    "        best_trial = study.best_trial\n",
    "        best_of_models.append({\n",
    "            \"model\": classifier_name,\n",
    "            \"model_score_params\": best_trial.params,\n",
    "            \"model_score_trial_number\": best_trial.number,\n",
    "            \"model_score_datetime\": best_trial.datetime_start,\n",
    "            \"model_score_duration\": best_trial.duration,\n",
    "            \"model_score_status\": best_trial.state,\n",
    "            \"model_score_key\": scoring,\n",
    "            \"model_score_value\": best_trial.value\n",
    "            \n",
    "        })\n",
    "\n",
    "        current_score = best_trial.value\n",
    "        \n",
    "        \n",
    "        if current_score and current_score > best_model_score:\n",
    "            best_model_score = best_trial.value\n",
    "            best_model = classifier_name\n",
    "            best_params = best_trial.params\n",
    "            \n",
    "        print(f\"Current Model: {classifier_name}, Current Score: {current_score} | Best Model: {best_model}, Best Score: {best_model_score}\")\n",
    "            \n",
    "        best_parameters_results = detailed_objective(classifier_name=classifier_name, trial=study.best_trial,  scoring=scoring)\n",
    "        print(f\"Best Parameters: {best_parameters_results}\")\n",
    "        \n",
    "\n",
    "    # Display all results and the best model\n",
    "    for result in best_of_models:\n",
    "        print(result)\n",
    "    \n",
    "    print(\"Best model:\", best_model)\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    \n",
    "    # Save the variables to a file\n",
    "    with open(\"best_model_and_params.pkl\", \"wb\") as f:\n",
    "        joblib.dump((best_model, best_params), f)\n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "    return best_model, best_params, current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:18:10,591] A new study created in memory with name: no-name-703fe0cb-578a-4c06-a455-ac66f7efb072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing model: RandomForest | Scoring: f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:18:13,714] Trial 0 finished with value: 0.4265402843601896 and parameters: {'n_estimators': 95, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 15, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': None}. Best is trial 0 with value: 0.4265402843601896.\n",
      "[I 2024-11-05 09:18:14,263] Trial 1 finished with value: 0.5393258426966292 and parameters: {'n_estimators': 136, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 2, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 1 with value: 0.5393258426966292.\n",
      "[I 2024-11-05 09:18:14,762] Trial 2 finished with value: 0.4462809917355372 and parameters: {'n_estimators': 67, 'max_depth': 10, 'min_samples_split': 17, 'min_samples_leaf': 19, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 20}. Best is trial 1 with value: 0.5393258426966292.\n",
      "[I 2024-11-05 09:18:15,952] Trial 3 finished with value: 0.4 and parameters: {'n_estimators': 253, 'max_depth': 20, 'min_samples_split': 20, 'min_samples_leaf': 7, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 27}. Best is trial 1 with value: 0.5393258426966292.\n",
      "[I 2024-11-05 09:18:17,024] Trial 4 finished with value: 0.2641509433962264 and parameters: {'n_estimators': 212, 'max_depth': 22, 'min_samples_split': 5, 'min_samples_leaf': 16, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_median', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 9}. Best is trial 1 with value: 0.5393258426966292.\n",
      "[I 2024-11-05 09:18:17,737] Trial 5 finished with value: 0.4375 and parameters: {'n_estimators': 214, 'max_depth': 20, 'min_samples_split': 17, 'min_samples_leaf': 11, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 1 with value: 0.5393258426966292.\n",
      "[I 2024-11-05 09:18:18,253] Trial 6 finished with value: 0.32558139534883723 and parameters: {'n_estimators': 128, 'max_depth': 28, 'min_samples_split': 6, 'min_samples_leaf': 9, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 3}. Best is trial 1 with value: 0.5393258426966292.\n",
      "[I 2024-11-05 09:18:18,813] Trial 7 finished with value: 0.5432098765432098 and parameters: {'n_estimators': 108, 'max_depth': 22, 'min_samples_split': 3, 'min_samples_leaf': 17, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 7 with value: 0.5432098765432098.\n",
      "[I 2024-11-05 09:18:19,218] Trial 8 finished with value: 0.36363636363636365 and parameters: {'n_estimators': 69, 'max_depth': 29, 'min_samples_split': 15, 'min_samples_leaf': 19, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 7 with value: 0.5432098765432098.\n",
      "[I 2024-11-05 09:18:20,100] Trial 9 finished with value: 0.475 and parameters: {'n_estimators': 222, 'max_depth': 18, 'min_samples_split': 13, 'min_samples_leaf': 12, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': None}. Best is trial 7 with value: 0.5432098765432098.\n",
      "[I 2024-11-05 09:18:21,301] Trial 10 finished with value: 0.5063291139240507 and parameters: {'n_estimators': 297, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 4, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 7 with value: 0.5432098765432098.\n",
      "[I 2024-11-05 09:18:21,881] Trial 11 finished with value: 0.4827586206896552 and parameters: {'n_estimators': 138, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 1, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 7 with value: 0.5432098765432098.\n",
      "[I 2024-11-05 09:18:22,406] Trial 12 finished with value: 0.5765765765765766 and parameters: {'n_estimators': 153, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 12 with value: 0.5765765765765766.\n",
      "[I 2024-11-05 09:18:22,915] Trial 13 finished with value: 0.48951048951048953 and parameters: {'n_estimators': 169, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 6, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 12 with value: 0.5765765765765766.\n",
      "[I 2024-11-05 09:18:23,668] Trial 14 finished with value: 0.5432098765432098 and parameters: {'n_estimators': 173, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 15, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 12 with value: 0.5765765765765766.\n",
      "[I 2024-11-05 09:18:24,091] Trial 15 finished with value: 0.5961538461538461 and parameters: {'n_estimators': 96, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 13, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 15 with value: 0.5961538461538461.\n",
      "[I 2024-11-05 09:18:24,579] Trial 16 finished with value: 0.5631067961165048 and parameters: {'n_estimators': 95, 'max_depth': 26, 'min_samples_split': 8, 'min_samples_leaf': 12, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 15 with value: 0.5961538461538461.\n",
      "[I 2024-11-05 09:18:25,251] Trial 17 finished with value: 0.594059405940594 and parameters: {'n_estimators': 168, 'max_depth': 15, 'min_samples_split': 13, 'min_samples_leaf': 9, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 15 with value: 0.5961538461538461.\n",
      "[I 2024-11-05 09:18:26,457] Trial 18 finished with value: 0.43902439024390244 and parameters: {'n_estimators': 197, 'max_depth': 16, 'min_samples_split': 13, 'min_samples_leaf': 9, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 30}. Best is trial 15 with value: 0.5961538461538461.\n",
      "[I 2024-11-05 09:18:26,839] Trial 19 finished with value: 0.5858585858585859 and parameters: {'n_estimators': 56, 'max_depth': 24, 'min_samples_split': 12, 'min_samples_leaf': 13, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 15 with value: 0.5961538461538461.\n",
      "[I 2024-11-05 09:18:27,765] Trial 20 finished with value: 0.6019417475728155 and parameters: {'n_estimators': 258, 'max_depth': 15, 'min_samples_split': 11, 'min_samples_leaf': 9, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 20 with value: 0.6019417475728155.\n",
      "[I 2024-11-05 09:18:28,572] Trial 21 finished with value: 0.6019417475728155 and parameters: {'n_estimators': 258, 'max_depth': 15, 'min_samples_split': 11, 'min_samples_leaf': 9, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 20 with value: 0.6019417475728155.\n",
      "[I 2024-11-05 09:18:29,560] Trial 22 finished with value: 0.58 and parameters: {'n_estimators': 265, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 7, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 20 with value: 0.6019417475728155.\n",
      "[I 2024-11-05 09:18:30,422] Trial 23 finished with value: 0.6226415094339622 and parameters: {'n_estimators': 256, 'max_depth': 18, 'min_samples_split': 11, 'min_samples_leaf': 10, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 23 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:18:31,301] Trial 24 finished with value: 0.5773195876288659 and parameters: {'n_estimators': 261, 'max_depth': 18, 'min_samples_split': 11, 'min_samples_leaf': 5, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 23 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:18:32,374] Trial 25 finished with value: 0.5961538461538461 and parameters: {'n_estimators': 294, 'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 10, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 23 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:18:33,768] Trial 26 finished with value: 0.2222222222222222 and parameters: {'n_estimators': 282, 'max_depth': 14, 'min_samples_split': 15, 'min_samples_leaf': 8, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 16}. Best is trial 23 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:18:34,630] Trial 27 finished with value: 0.5904761904761905 and parameters: {'n_estimators': 241, 'max_depth': 18, 'min_samples_split': 11, 'min_samples_leaf': 11, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 23 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:18:35,464] Trial 28 finished with value: 0.5684210526315789 and parameters: {'n_estimators': 235, 'max_depth': 17, 'min_samples_split': 9, 'min_samples_leaf': 5, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 23 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:18:36,213] Trial 29 finished with value: 0.4752475247524752 and parameters: {'n_estimators': 268, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 14, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 23 with value: 0.6226415094339622.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_name: RandomForest | study.best_trial.value: 0.6226415094339622\n",
      "Current Model: RandomForest, Current Score: 0.6226415094339622 | Best Model: RandomForest, Best Score: 0.6226415094339622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:18:37,146] A new study created in memory with name: no-name-ac97d0e7-13b1-42a7-a78f-c4975dc0a5c6\n",
      "[I 2024-11-05 09:18:37,291] Trial 0 finished with value: 0.6428571428571429 and parameters: {'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 10, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 0 with value: 0.6428571428571429.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized RandomForest pipeline and test metrics to RandomForest_pipeline.pkl\n",
      "Best Parameters: {'classifier_name': 'RandomForest', 'f1': 0.6226415094339622, 'accuracy': 0.8, 'precision': 0.5789473684210527, 'recall': 0.673469387755102, 'roc_auc': 0.8084876334639816, 'score_training_f1': 0.6063200723503779, 'best_params': FrozenTrial(number=23, state=1, values=[0.6226415094339622], datetime_start=datetime.datetime(2024, 11, 5, 9, 18, 29, 561301), datetime_complete=datetime.datetime(2024, 11, 5, 9, 18, 30, 422707), params={'n_estimators': 256, 'max_depth': 18, 'min_samples_split': 11, 'min_samples_leaf': 10, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_estimators': IntDistribution(high=300, log=False, low=50, step=1), 'max_depth': IntDistribution(high=30, log=False, low=2, step=1), 'min_samples_split': IntDistribution(high=20, log=False, low=2, step=1), 'min_samples_leaf': IntDistribution(high=20, log=False, low=1, step=1), 'categorical_strategy': CategoricalDistribution(choices=('most_frequent', 'constant')), 'numerical_strategy': CategoricalDistribution(choices=('mean', 'median', 'most_frequent')), 'outlier_strategy': CategoricalDistribution(choices=('power_transform', 'log_transform', 'iqr_clip', 'iqr_median', 'iqr_mean')), 'resampler': CategoricalDistribution(choices=('RandomOverSampler', 'SMOTEENN', 'SMOTETomek')), 'scaler': CategoricalDistribution(choices=('minmax', 'standard', 'robust')), 'dim_red': CategoricalDistribution(choices=('PCA', None))}, trial_id=23, value=None), 'classification_report': {'0': {'precision': 0.8881118881118881, 'recall': 0.8410596026490066, 'f1-score': 0.8639455782312925, 'support': 151.0}, '1': {'precision': 0.5789473684210527, 'recall': 0.673469387755102, 'f1-score': 0.6226415094339622, 'support': 49.0}, 'accuracy': 0.8, 'macro avg': {'precision': 0.7335296282664704, 'recall': 0.7572644952020543, 'f1-score': 0.7432935438326274, 'support': 200.0}, 'weighted avg': {'precision': 0.8123665807876336, 'recall': 0.8, 'f1-score': 0.8048260813759466, 'support': 200.0}}}\n",
      "Optimizing model: DecisionTree | Scoring: f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:18:37,507] Trial 1 finished with value: 0.6105263157894737 and parameters: {'max_depth': 25, 'min_samples_split': 8, 'min_samples_leaf': 7, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': None}. Best is trial 0 with value: 0.6428571428571429.\n",
      "[I 2024-11-05 09:18:37,719] Trial 2 finished with value: 0.5031446540880503 and parameters: {'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 2, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 12}. Best is trial 0 with value: 0.6428571428571429.\n",
      "[I 2024-11-05 09:18:37,972] Trial 3 finished with value: 0.3302752293577982 and parameters: {'max_depth': 25, 'min_samples_split': 16, 'min_samples_leaf': 14, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 6}. Best is trial 0 with value: 0.6428571428571429.\n",
      "[I 2024-11-05 09:18:38,155] Trial 4 finished with value: 0.6607142857142857 and parameters: {'max_depth': 25, 'min_samples_split': 17, 'min_samples_leaf': 15, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:38,304] Trial 5 finished with value: 0.5084745762711864 and parameters: {'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 15, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:38,439] Trial 6 finished with value: 0.6504065040650406 and parameters: {'max_depth': 30, 'min_samples_split': 19, 'min_samples_leaf': 17, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_median', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:38,559] Trial 7 finished with value: 0.6446280991735537 and parameters: {'max_depth': 28, 'min_samples_split': 20, 'min_samples_leaf': 20, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:38,700] Trial 8 finished with value: 0.2653061224489796 and parameters: {'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 13, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 10}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:38,895] Trial 9 finished with value: 0.5742574257425742 and parameters: {'max_depth': 21, 'min_samples_split': 6, 'min_samples_leaf': 16, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:39,107] Trial 10 finished with value: 0.0 and parameters: {'max_depth': 17, 'min_samples_split': 14, 'min_samples_leaf': 8, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 29}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:39,248] Trial 11 finished with value: 0.6446280991735537 and parameters: {'max_depth': 30, 'min_samples_split': 20, 'min_samples_leaf': 20, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_median', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:39,382] Trial 12 finished with value: 0.6504065040650406 and parameters: {'max_depth': 20, 'min_samples_split': 16, 'min_samples_leaf': 17, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:39,495] Trial 13 finished with value: 0.6037735849056604 and parameters: {'max_depth': 25, 'min_samples_split': 13, 'min_samples_leaf': 12, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_median', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 4 with value: 0.6607142857142857.\n",
      "[I 2024-11-05 09:18:39,612] Trial 14 finished with value: 0.6666666666666666 and parameters: {'max_depth': 30, 'min_samples_split': 17, 'min_samples_leaf': 18, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:39,734] Trial 15 finished with value: 0.6666666666666666 and parameters: {'max_depth': 21, 'min_samples_split': 17, 'min_samples_leaf': 18, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:39,869] Trial 16 finished with value: 0.5967741935483871 and parameters: {'max_depth': 13, 'min_samples_split': 12, 'min_samples_leaf': 19, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:40,033] Trial 17 finished with value: 0.5981308411214953 and parameters: {'max_depth': 20, 'min_samples_split': 17, 'min_samples_leaf': 1, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:40,208] Trial 18 finished with value: 0.06153846153846154 and parameters: {'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 18, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 25}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:40,503] Trial 19 finished with value: 0.6213592233009708 and parameters: {'max_depth': 10, 'min_samples_split': 14, 'min_samples_leaf': 5, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:40,708] Trial 20 finished with value: 0.6296296296296297 and parameters: {'max_depth': 22, 'min_samples_split': 18, 'min_samples_leaf': 10, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:40,874] Trial 21 finished with value: 0.6607142857142857 and parameters: {'max_depth': 27, 'min_samples_split': 16, 'min_samples_leaf': 15, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:41,043] Trial 22 finished with value: 0.6666666666666666 and parameters: {'max_depth': 23, 'min_samples_split': 18, 'min_samples_leaf': 18, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:41,196] Trial 23 finished with value: 0.6324786324786325 and parameters: {'max_depth': 23, 'min_samples_split': 15, 'min_samples_leaf': 18, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:41,332] Trial 24 finished with value: 0.6666666666666666 and parameters: {'max_depth': 18, 'min_samples_split': 18, 'min_samples_leaf': 20, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:41,548] Trial 25 finished with value: 0.6181818181818182 and parameters: {'max_depth': 28, 'min_samples_split': 11, 'min_samples_leaf': 12, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:41,696] Trial 26 finished with value: 0.22 and parameters: {'max_depth': 19, 'min_samples_split': 18, 'min_samples_leaf': 18, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 19}. Best is trial 14 with value: 0.6666666666666666.\n",
      "[I 2024-11-05 09:18:41,851] Trial 27 finished with value: 0.7079646017699115 and parameters: {'max_depth': 22, 'min_samples_split': 14, 'min_samples_leaf': 16, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 27 with value: 0.7079646017699115.\n",
      "[I 2024-11-05 09:18:42,050] Trial 28 finished with value: 0.7027027027027027 and parameters: {'max_depth': 15, 'min_samples_split': 14, 'min_samples_leaf': 16, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': None}. Best is trial 27 with value: 0.7079646017699115.\n",
      "[I 2024-11-05 09:18:42,207] Trial 29 finished with value: 0.625 and parameters: {'max_depth': 8, 'min_samples_split': 13, 'min_samples_leaf': 12, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': None}. Best is trial 27 with value: 0.7079646017699115.\n",
      "[I 2024-11-05 09:18:42,335] A new study created in memory with name: no-name-95401830-a910-4d42-b865-cae57fb26cc3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_name: DecisionTree | study.best_trial.value: 0.7079646017699115\n",
      "Current Model: DecisionTree, Current Score: 0.7079646017699115 | Best Model: DecisionTree, Best Score: 0.7079646017699115\n",
      "Serialized DecisionTree pipeline and test metrics to DecisionTree_pipeline.pkl\n",
      "Best Parameters: {'classifier_name': 'DecisionTree', 'f1': 0.7079646017699115, 'accuracy': 0.835, 'precision': 0.625, 'recall': 0.8163265306122449, 'roc_auc': 0.8525476415731856, 'score_training_f1': 0.6628547996113652, 'best_params': FrozenTrial(number=27, state=1, values=[0.7079646017699115], datetime_start=datetime.datetime(2024, 11, 5, 9, 18, 41, 697596), datetime_complete=datetime.datetime(2024, 11, 5, 9, 18, 41, 851680), params={'max_depth': 22, 'min_samples_split': 14, 'min_samples_leaf': 16, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'max_depth': IntDistribution(high=30, log=False, low=2, step=1), 'min_samples_split': IntDistribution(high=20, log=False, low=2, step=1), 'min_samples_leaf': IntDistribution(high=20, log=False, low=1, step=1), 'categorical_strategy': CategoricalDistribution(choices=('most_frequent', 'constant')), 'numerical_strategy': CategoricalDistribution(choices=('mean', 'median', 'most_frequent')), 'outlier_strategy': CategoricalDistribution(choices=('power_transform', 'log_transform', 'iqr_clip', 'iqr_median', 'iqr_mean')), 'resampler': CategoricalDistribution(choices=('RandomOverSampler', 'SMOTEENN', 'SMOTETomek')), 'scaler': CategoricalDistribution(choices=('minmax', 'standard', 'robust')), 'dim_red': CategoricalDistribution(choices=('PCA', None))}, trial_id=27, value=None), 'classification_report': {'0': {'precision': 0.9338235294117647, 'recall': 0.8410596026490066, 'f1-score': 0.8850174216027874, 'support': 151.0}, '1': {'precision': 0.625, 'recall': 0.8163265306122449, 'f1-score': 0.7079646017699115, 'support': 49.0}, 'accuracy': 0.835, 'macro avg': {'precision': 0.7794117647058824, 'recall': 0.8286930666306258, 'f1-score': 0.7964910116863495, 'support': 200.0}, 'weighted avg': {'precision': 0.8581617647058823, 'recall': 0.835, 'f1-score': 0.8416394807437328, 'support': 200.0}}}\n",
      "Optimizing model: XGBoost | Scoring: f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:18:43,446] Trial 0 finished with value: 0.3878787878787879 and parameters: {'booster': 'dart', 'lambda': 0.0063018989280880545, 'alpha': 2.299475764751072e-05, 'subsample': 0.5883416785342259, 'colsample_bytree': 0.21022465167248694, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 4}. Best is trial 0 with value: 0.3878787878787879.\n",
      "[I 2024-11-05 09:18:43,779] Trial 1 finished with value: 0.625 and parameters: {'booster': 'gbtree', 'lambda': 6.892523464636765e-05, 'alpha': 1.634601319073008e-05, 'subsample': 0.8783831980375796, 'colsample_bytree': 0.7742793434719031, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.625.\n",
      "[I 2024-11-05 09:18:45,554] Trial 2 finished with value: 0.37583892617449666 and parameters: {'booster': 'dart', 'lambda': 0.007287904797633619, 'alpha': 3.1849380614600745e-08, 'subsample': 0.21658638310493705, 'colsample_bytree': 0.8164384531320152, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 15}. Best is trial 1 with value: 0.625.\n",
      "[I 2024-11-05 09:18:47,352] Trial 3 finished with value: 0.3787878787878788 and parameters: {'booster': 'dart', 'lambda': 4.1886427021623545e-05, 'alpha': 0.0009608864323615093, 'subsample': 0.7919612142383159, 'colsample_bytree': 0.5349307441454727, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 26}. Best is trial 1 with value: 0.625.\n",
      "[I 2024-11-05 09:18:47,665] Trial 4 finished with value: 0.56 and parameters: {'booster': 'gblinear', 'lambda': 2.557604331217606e-06, 'alpha': 4.847953528363775e-08, 'subsample': 0.4749706455682435, 'colsample_bytree': 0.7674408142171487, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 1 with value: 0.625.\n",
      "[I 2024-11-05 09:18:48,166] Trial 5 finished with value: 0.38095238095238093 and parameters: {'booster': 'gbtree', 'lambda': 1.038451191313982e-07, 'alpha': 0.0002832493011207187, 'subsample': 0.4931417688331874, 'colsample_bytree': 0.9294363874207792, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 3}. Best is trial 1 with value: 0.625.\n",
      "[I 2024-11-05 09:18:48,404] Trial 6 finished with value: 0.42528735632183906 and parameters: {'booster': 'gblinear', 'lambda': 2.651323937801454e-08, 'alpha': 7.29562765701299e-07, 'subsample': 0.7647074069002489, 'colsample_bytree': 0.2440678656846288, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 26}. Best is trial 1 with value: 0.625.\n",
      "[I 2024-11-05 09:18:48,676] Trial 7 finished with value: 0.5578231292517006 and parameters: {'booster': 'gblinear', 'lambda': 0.19600302504002712, 'alpha': 0.032773812692484604, 'subsample': 0.35257729028230883, 'colsample_bytree': 0.340722785380766, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': None}. Best is trial 1 with value: 0.625.\n",
      "[I 2024-11-05 09:18:49,026] Trial 8 finished with value: 0.5241379310344828 and parameters: {'booster': 'gblinear', 'lambda': 0.07782900876925747, 'alpha': 6.54293718992126e-06, 'subsample': 0.3125007082027845, 'colsample_bytree': 0.37750167282844993, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 14}. Best is trial 1 with value: 0.625.\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2024-11-05 09:18:49,229] Trial 9 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 1.5557475812885146e-06, 'alpha': 0.4011346007812831, 'subsample': 0.906405414632637, 'colsample_bytree': 0.9525671938963092, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.625.\n",
      "[I 2024-11-05 09:18:49,556] Trial 10 finished with value: 0.631578947368421 and parameters: {'booster': 'gbtree', 'lambda': 0.000722515323701144, 'alpha': 0.005405371833898261, 'subsample': 0.9806726429972314, 'colsample_bytree': 0.634094370059548, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:49,860] Trial 11 finished with value: 0.5806451612903226 and parameters: {'booster': 'gbtree', 'lambda': 0.0003960085455999221, 'alpha': 0.005137290360065879, 'subsample': 0.9966026453946112, 'colsample_bytree': 0.617925901748891, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:50,217] Trial 12 finished with value: 0.5494505494505495 and parameters: {'booster': 'gbtree', 'lambda': 0.000141610875820154, 'alpha': 0.020458102365037974, 'subsample': 0.8005232863696744, 'colsample_bytree': 0.6386179570618247, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:50,554] Trial 13 finished with value: 0.5894736842105263 and parameters: {'booster': 'gbtree', 'lambda': 0.002467625090092106, 'alpha': 0.0001003884046804586, 'subsample': 0.9821742567628186, 'colsample_bytree': 0.7397171946124246, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:50,914] Trial 14 finished with value: 0.4470588235294118 and parameters: {'booster': 'gbtree', 'lambda': 9.693302678085502e-06, 'alpha': 8.215408579225118e-07, 'subsample': 0.6835585430698699, 'colsample_bytree': 0.47101419232147657, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:51,300] Trial 15 finished with value: 0.5494505494505495 and parameters: {'booster': 'gbtree', 'lambda': 0.0009837766575627723, 'alpha': 0.9570705820706545, 'subsample': 0.8839117590410501, 'colsample_bytree': 0.6891357954584649, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:51,682] Trial 16 finished with value: 0.5894736842105263 and parameters: {'booster': 'gbtree', 'lambda': 6.454282587030185e-05, 'alpha': 0.002028154474428624, 'subsample': 0.8922731436976756, 'colsample_bytree': 0.8540260780346223, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:52,039] Trial 17 finished with value: 0.5979381443298969 and parameters: {'booster': 'gbtree', 'lambda': 0.027007642741087037, 'alpha': 1.3936507145729786e-05, 'subsample': 0.6644902842373525, 'colsample_bytree': 0.552330126557082, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:52,416] Trial 18 finished with value: 0.5894736842105263 and parameters: {'booster': 'gbtree', 'lambda': 6.124677481401286e-07, 'alpha': 0.04866847304987602, 'subsample': 0.8584249409192817, 'colsample_bytree': 0.8632722116643109, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:52,739] Trial 19 finished with value: 0.5714285714285714 and parameters: {'booster': 'gbtree', 'lambda': 1.3074284441685331e-05, 'alpha': 7.676433434751739e-07, 'subsample': 0.9958274182179246, 'colsample_bytree': 0.6995582054549657, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:56,895] Trial 20 finished with value: 0.4050632911392405 and parameters: {'booster': 'dart', 'lambda': 0.00044620411296524795, 'alpha': 0.00034343121005979925, 'subsample': 0.7058516272787403, 'colsample_bytree': 0.4550529816167459, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:57,252] Trial 21 finished with value: 0.47619047619047616 and parameters: {'booster': 'gbtree', 'lambda': 0.8648594385206406, 'alpha': 1.563103805930863e-05, 'subsample': 0.6150336548615407, 'colsample_bytree': 0.5614504440164962, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:57,620] Trial 22 finished with value: 0.5274725274725275 and parameters: {'booster': 'gbtree', 'lambda': 0.042000621979161774, 'alpha': 5.7628277006555334e-06, 'subsample': 0.9296822435180272, 'colsample_bytree': 0.5139027951800172, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:57,925] Trial 23 finished with value: 0.5057471264367817 and parameters: {'booster': 'gbtree', 'lambda': 0.019229531577923883, 'alpha': 4.964263569809274e-05, 'subsample': 0.7043599845043729, 'colsample_bytree': 0.6422963791562137, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:58,330] Trial 24 finished with value: 0.5656565656565656 and parameters: {'booster': 'gbtree', 'lambda': 0.002074762392343938, 'alpha': 2.5926738900416276e-06, 'subsample': 0.8047130773272352, 'colsample_bytree': 0.7764823084926, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:58,703] Trial 25 finished with value: 0.5333333333333333 and parameters: {'booster': 'gbtree', 'lambda': 0.3486179850348608, 'alpha': 0.00012106925232060593, 'subsample': 0.5800715572427765, 'colsample_bytree': 0.579509506896012, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:59,036] Trial 26 finished with value: 0.5747126436781609 and parameters: {'booster': 'gbtree', 'lambda': 0.0002924969952215722, 'alpha': 0.0026748718704693395, 'subsample': 0.8369239134945711, 'colsample_bytree': 0.7017041415964865, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:59,390] Trial 27 finished with value: 0.5806451612903226 and parameters: {'booster': 'gbtree', 'lambda': 2.850556149807164e-05, 'alpha': 2.940771127295646e-07, 'subsample': 0.9491106220557076, 'colsample_bytree': 0.4381639304102103, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:18:59,728] Trial 28 finished with value: 0.5833333333333334 and parameters: {'booster': 'gbtree', 'lambda': 0.014750629794179237, 'alpha': 0.0003855719510813183, 'subsample': 0.7449266499278275, 'colsample_bytree': 0.9880880512536707, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 10 with value: 0.631578947368421.\n",
      "[I 2024-11-05 09:19:02,691] Trial 29 finished with value: 0.4077669902912621 and parameters: {'booster': 'dart', 'lambda': 0.005724299313975023, 'alpha': 1.8335974702669376e-05, 'subsample': 0.58686175180452, 'colsample_bytree': 0.6597769883857344, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 29}. Best is trial 10 with value: 0.631578947368421.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_name: XGBoost | study.best_trial.value: 0.631578947368421\n",
      "Current Model: XGBoost, Current Score: 0.631578947368421 | Best Model: DecisionTree, Best Score: 0.7079646017699115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:19:03,101] A new study created in memory with name: no-name-a130db3b-1400-4cc5-bcf6-c995879514d6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized XGBoost pipeline and test metrics to XGBoost_pipeline.pkl\n",
      "Best Parameters: {'classifier_name': 'XGBoost', 'f1': 0.631578947368421, 'accuracy': 0.825, 'precision': 0.6521739130434783, 'recall': 0.6122448979591837, 'roc_auc': 0.8121367752398972, 'score_training_f1': 0.598801024882597, 'best_params': FrozenTrial(number=10, state=1, values=[0.631578947368421], datetime_start=datetime.datetime(2024, 11, 5, 9, 18, 49, 229613), datetime_complete=datetime.datetime(2024, 11, 5, 9, 18, 49, 556521), params={'booster': 'gbtree', 'lambda': 0.000722515323701144, 'alpha': 0.005405371833898261, 'subsample': 0.9806726429972314, 'colsample_bytree': 0.634094370059548, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'booster': CategoricalDistribution(choices=('gbtree', 'gblinear', 'dart')), 'lambda': FloatDistribution(high=1.0, log=True, low=1e-08, step=None), 'alpha': FloatDistribution(high=1.0, log=True, low=1e-08, step=None), 'subsample': FloatDistribution(high=1.0, log=False, low=0.2, step=None), 'colsample_bytree': FloatDistribution(high=1.0, log=False, low=0.2, step=None), 'categorical_strategy': CategoricalDistribution(choices=('most_frequent', 'constant')), 'numerical_strategy': CategoricalDistribution(choices=('mean', 'median', 'most_frequent')), 'outlier_strategy': CategoricalDistribution(choices=('power_transform', 'log_transform', 'iqr_clip', 'iqr_median', 'iqr_mean')), 'resampler': CategoricalDistribution(choices=('RandomOverSampler', 'SMOTEENN', 'SMOTETomek')), 'scaler': CategoricalDistribution(choices=('minmax', 'standard', 'robust')), 'dim_red': CategoricalDistribution(choices=('PCA', None))}, trial_id=10, value=None), 'classification_report': {'0': {'precision': 0.8766233766233766, 'recall': 0.8940397350993378, 'f1-score': 0.8852459016393442, 'support': 151.0}, '1': {'precision': 0.6521739130434783, 'recall': 0.6122448979591837, 'f1-score': 0.631578947368421, 'support': 49.0}, 'accuracy': 0.825, 'macro avg': {'precision': 0.7643986448334275, 'recall': 0.7531423165292608, 'f1-score': 0.7584124245038826, 'support': 200.0}, 'weighted avg': {'precision': 0.8216332580463015, 'recall': 0.825, 'f1-score': 0.8230974978429682, 'support': 200.0}}}\n",
      "Optimizing model: LGBM | Scoring: f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:19:09,092] Trial 0 finished with value: 0.37583892617449666 and parameters: {'lambda_l1': 4.192057199912162e-07, 'lambda_l2': 1.4971908813890094e-08, 'num_leaves': 120, 'feature_fraction': 0.8349603575152886, 'bagging_fraction': 0.6521221627526365, 'bagging_freq': 4, 'min_child_samples': 6, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 4}. Best is trial 0 with value: 0.37583892617449666.\n",
      "[I 2024-11-05 09:19:10,521] Trial 1 finished with value: 0.6458333333333334 and parameters: {'lambda_l1': 0.025973676145411313, 'lambda_l2': 0.0024882302541196833, 'num_leaves': 16, 'feature_fraction': 0.6878783689956582, 'bagging_fraction': 0.6410058883783913, 'bagging_freq': 6, 'min_child_samples': 73, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:11,312] Trial 2 finished with value: 0.35428571428571426 and parameters: {'lambda_l1': 1.7920572503383054e-05, 'lambda_l2': 3.3064022458797853e-07, 'num_leaves': 113, 'feature_fraction': 0.8396952018089057, 'bagging_fraction': 0.4377325743164203, 'bagging_freq': 1, 'min_child_samples': 78, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 15}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:12,383] Trial 3 finished with value: 0.36220472440944884 and parameters: {'lambda_l1': 4.731708022730327e-08, 'lambda_l2': 1.4931046936215234e-08, 'num_leaves': 195, 'feature_fraction': 0.6716554986951986, 'bagging_fraction': 0.7737004045917937, 'bagging_freq': 6, 'min_child_samples': 45, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 26}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:13,274] Trial 4 finished with value: 0.5245901639344263 and parameters: {'lambda_l1': 5.241821044693493e-06, 'lambda_l2': 8.789942576497343e-05, 'num_leaves': 25, 'feature_fraction': 0.5805875019045603, 'bagging_fraction': 0.45141688360803206, 'bagging_freq': 3, 'min_child_samples': 73, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:14,292] Trial 5 finished with value: 0.44036697247706424 and parameters: {'lambda_l1': 5.5027126246963025, 'lambda_l2': 2.464469628836511, 'num_leaves': 216, 'feature_fraction': 0.4762289566668396, 'bagging_fraction': 0.7339126635230464, 'bagging_freq': 3, 'min_child_samples': 92, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 3}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:17,257] Trial 6 finished with value: 0.4444444444444444 and parameters: {'lambda_l1': 0.0009668196206307308, 'lambda_l2': 0.04234335309344587, 'num_leaves': 167, 'feature_fraction': 0.4317597094688424, 'bagging_fraction': 0.5397296995775732, 'bagging_freq': 5, 'min_child_samples': 10, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 26}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:21,726] Trial 7 finished with value: 0.5591397849462365 and parameters: {'lambda_l1': 5.074792267310124e-08, 'lambda_l2': 3.7117551745148694, 'num_leaves': 182, 'feature_fraction': 0.9469197080614499, 'bagging_fraction': 0.8886645225490304, 'bagging_freq': 2, 'min_child_samples': 21, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:24,552] Trial 8 finished with value: 0.42718446601941745 and parameters: {'lambda_l1': 0.0024937386604957368, 'lambda_l2': 1.329924115572129, 'num_leaves': 137, 'feature_fraction': 0.9168356124480066, 'bagging_fraction': 0.6111829563423964, 'bagging_freq': 1, 'min_child_samples': 26, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 14}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:26,465] Trial 9 finished with value: 0.47058823529411764 and parameters: {'lambda_l1': 5.5244112740718076e-05, 'lambda_l2': 0.0001067684638192519, 'num_leaves': 8, 'feature_fraction': 0.5643954350846495, 'bagging_fraction': 0.9702467593494969, 'bagging_freq': 7, 'min_child_samples': 95, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:28,534] Trial 10 finished with value: 0.6391752577319587 and parameters: {'lambda_l1': 0.1248458921190857, 'lambda_l2': 0.00607217348769717, 'num_leaves': 64, 'feature_fraction': 0.7419287462533185, 'bagging_fraction': 0.7972758920990051, 'bagging_freq': 7, 'min_child_samples': 53, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:30,579] Trial 11 finished with value: 0.6170212765957447 and parameters: {'lambda_l1': 0.7023528119297541, 'lambda_l2': 0.006528393153702216, 'num_leaves': 59, 'feature_fraction': 0.7152943640231929, 'bagging_fraction': 0.820070557374896, 'bagging_freq': 7, 'min_child_samples': 59, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:32,360] Trial 12 finished with value: 0.6067415730337079 and parameters: {'lambda_l1': 0.046315194058237016, 'lambda_l2': 0.004400854711275384, 'num_leaves': 61, 'feature_fraction': 0.7464761404093542, 'bagging_fraction': 0.5753103345647084, 'bagging_freq': 6, 'min_child_samples': 50, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:34,178] Trial 13 finished with value: 0.6304347826086957 and parameters: {'lambda_l1': 0.023676314931704027, 'lambda_l2': 3.588270850479139e-05, 'num_leaves': 62, 'feature_fraction': 0.633570979600123, 'bagging_fraction': 0.8417178771629648, 'bagging_freq': 6, 'min_child_samples': 66, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:36,723] Trial 14 finished with value: 0.6222222222222222 and parameters: {'lambda_l1': 0.12827104835413536, 'lambda_l2': 0.0738371625545339, 'num_leaves': 38, 'feature_fraction': 0.8087296444890989, 'bagging_fraction': 0.6874365351455984, 'bagging_freq': 5, 'min_child_samples': 36, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:38,579] Trial 15 finished with value: 0.5918367346938775 and parameters: {'lambda_l1': 0.005679167080314431, 'lambda_l2': 5.2277905804553965e-06, 'num_leaves': 85, 'feature_fraction': 0.7612885733382029, 'bagging_fraction': 0.9078742153441601, 'bagging_freq': 7, 'min_child_samples': 80, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 1 with value: 0.6458333333333334.\n",
      "[I 2024-11-05 09:19:40,086] Trial 16 finished with value: 0.6862745098039216 and parameters: {'lambda_l1': 6.250272121276135, 'lambda_l2': 0.0021764551967213804, 'num_leaves': 92, 'feature_fraction': 0.5835673288125324, 'bagging_fraction': 0.7373937055360235, 'bagging_freq': 5, 'min_child_samples': 59, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 16 with value: 0.6862745098039216.\n",
      "[I 2024-11-05 09:19:41,570] Trial 17 finished with value: 0.7027027027027027 and parameters: {'lambda_l1': 9.543594528095202, 'lambda_l2': 0.0009994744081232951, 'num_leaves': 247, 'feature_fraction': 0.5235056313799831, 'bagging_fraction': 0.7108377878773701, 'bagging_freq': 5, 'min_child_samples': 64, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 17 with value: 0.7027027027027027.\n",
      "[I 2024-11-05 09:19:43,640] Trial 18 finished with value: 0.6909090909090909 and parameters: {'lambda_l1': 8.405380346491068, 'lambda_l2': 5.065907887139368e-06, 'num_leaves': 253, 'feature_fraction': 0.5229065625730351, 'bagging_fraction': 0.7315782293312849, 'bagging_freq': 4, 'min_child_samples': 40, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 17 with value: 0.7027027027027027.\n",
      "[I 2024-11-05 09:19:46,117] Trial 19 finished with value: 0.6262626262626263 and parameters: {'lambda_l1': 0.7707035857850326, 'lambda_l2': 2.51659110511156e-06, 'num_leaves': 254, 'feature_fraction': 0.48041348871374634, 'bagging_fraction': 0.5101566266629934, 'bagging_freq': 4, 'min_child_samples': 39, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 17 with value: 0.7027027027027027.\n",
      "[I 2024-11-05 09:19:48,871] Trial 20 finished with value: 0.43373493975903615 and parameters: {'lambda_l1': 0.9810533843328617, 'lambda_l2': 4.321874458632161e-07, 'num_leaves': 244, 'feature_fraction': 0.5342124477442085, 'bagging_fraction': 0.7082813730170929, 'bagging_freq': 3, 'min_child_samples': 41, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': None}. Best is trial 17 with value: 0.7027027027027027.\n",
      "[I 2024-11-05 09:19:50,587] Trial 21 finished with value: 0.6851851851851852 and parameters: {'lambda_l1': 7.716707575568698, 'lambda_l2': 1.2790503007193077e-05, 'num_leaves': 228, 'feature_fraction': 0.6155356216695069, 'bagging_fraction': 0.7446583293716934, 'bagging_freq': 5, 'min_child_samples': 56, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 17 with value: 0.7027027027027027.\n",
      "[I 2024-11-05 09:19:52,096] Trial 22 finished with value: 0.6851851851851852 and parameters: {'lambda_l1': 8.397776612750878, 'lambda_l2': 0.0004475580121430176, 'num_leaves': 151, 'feature_fraction': 0.5103232965824125, 'bagging_fraction': 0.6863182812641266, 'bagging_freq': 4, 'min_child_samples': 63, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 17 with value: 0.7027027027027027.\n",
      "[I 2024-11-05 09:19:53,325] Trial 23 finished with value: 0.6732673267326733 and parameters: {'lambda_l1': 1.3464822156048541, 'lambda_l2': 0.0005417590912217002, 'num_leaves': 215, 'feature_fraction': 0.4156662654915587, 'bagging_fraction': 0.8508987087055337, 'bagging_freq': 5, 'min_child_samples': 86, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 17 with value: 0.7027027027027027.\n",
      "[I 2024-11-05 09:19:56,742] Trial 24 finished with value: 0.5111111111111111 and parameters: {'lambda_l1': 0.19523833682902292, 'lambda_l2': 6.025991030736816e-07, 'num_leaves': 97, 'feature_fraction': 0.5967478341809861, 'bagging_fraction': 0.7729571917446129, 'bagging_freq': 5, 'min_child_samples': 34, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 17 with value: 0.7027027027027027.\n",
      "[I 2024-11-05 09:19:58,049] Trial 25 finished with value: 0.6730769230769231 and parameters: {'lambda_l1': 2.372828531584838, 'lambda_l2': 0.08832766755147316, 'num_leaves': 237, 'feature_fraction': 0.5313844574942488, 'bagging_fraction': 0.6131660777940517, 'bagging_freq': 4, 'min_child_samples': 68, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 17 with value: 0.7027027027027027.\n",
      "[I 2024-11-05 09:19:59,698] Trial 26 finished with value: 0.7037037037037037 and parameters: {'lambda_l1': 8.640108456439672, 'lambda_l2': 0.0011336127430597253, 'num_leaves': 205, 'feature_fraction': 0.6447478000833163, 'bagging_fraction': 0.6667583548419981, 'bagging_freq': 3, 'min_child_samples': 49, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 26 with value: 0.7037037037037037.\n",
      "[I 2024-11-05 09:20:02,784] Trial 27 finished with value: 0.5714285714285714 and parameters: {'lambda_l1': 0.3255915804933514, 'lambda_l2': 0.0003893724572417336, 'num_leaves': 203, 'feature_fraction': 0.6448509291252729, 'bagging_fraction': 0.5615735731163699, 'bagging_freq': 2, 'min_child_samples': 28, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 26 with value: 0.7037037037037037.\n",
      "[I 2024-11-05 09:20:05,522] Trial 28 finished with value: 0.5934065934065934 and parameters: {'lambda_l1': 0.0001741342970290238, 'lambda_l2': 2.3410166004190287e-05, 'num_leaves': 227, 'feature_fraction': 0.4555414748199664, 'bagging_fraction': 0.6605443634959328, 'bagging_freq': 2, 'min_child_samples': 46, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 26 with value: 0.7037037037037037.\n",
      "[I 2024-11-05 09:20:11,006] Trial 29 finished with value: 0.27419354838709675 and parameters: {'lambda_l1': 0.007653791753091383, 'lambda_l2': 5.578490111656419e-08, 'num_leaves': 256, 'feature_fraction': 0.53904464872844, 'bagging_fraction': 0.6529845604038373, 'bagging_freq': 3, 'min_child_samples': 12, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 9}. Best is trial 26 with value: 0.7037037037037037.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_name: LGBM | study.best_trial.value: 0.7037037037037037\n",
      "Current Model: LGBM, Current Score: 0.7037037037037037 | Best Model: DecisionTree, Best Score: 0.7079646017699115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:20:12,546] A new study created in memory with name: no-name-bed99eca-5cd2-4caf-8bbf-e15d168a006b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized LGBM pipeline and test metrics to LGBM_pipeline.pkl\n",
      "Best Parameters: {'classifier_name': 'LGBM', 'f1': 0.7037037037037037, 'accuracy': 0.84, 'precision': 0.6440677966101694, 'recall': 0.7755102040816326, 'roc_auc': 0.8286254899310718, 'score_training_f1': 0.7253300796012299, 'best_params': FrozenTrial(number=26, state=1, values=[0.7037037037037037], datetime_start=datetime.datetime(2024, 11, 5, 9, 19, 58, 49931), datetime_complete=datetime.datetime(2024, 11, 5, 9, 19, 59, 698591), params={'lambda_l1': 8.640108456439672, 'lambda_l2': 0.0011336127430597253, 'num_leaves': 205, 'feature_fraction': 0.6447478000833163, 'bagging_fraction': 0.6667583548419981, 'bagging_freq': 3, 'min_child_samples': 49, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lambda_l1': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'lambda_l2': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'num_leaves': IntDistribution(high=256, log=False, low=2, step=1), 'feature_fraction': FloatDistribution(high=1.0, log=False, low=0.4, step=None), 'bagging_fraction': FloatDistribution(high=1.0, log=False, low=0.4, step=None), 'bagging_freq': IntDistribution(high=7, log=False, low=1, step=1), 'min_child_samples': IntDistribution(high=100, log=False, low=5, step=1), 'categorical_strategy': CategoricalDistribution(choices=('most_frequent', 'constant')), 'numerical_strategy': CategoricalDistribution(choices=('mean', 'median', 'most_frequent')), 'outlier_strategy': CategoricalDistribution(choices=('power_transform', 'log_transform', 'iqr_clip', 'iqr_median', 'iqr_mean')), 'resampler': CategoricalDistribution(choices=('RandomOverSampler', 'SMOTEENN', 'SMOTETomek')), 'scaler': CategoricalDistribution(choices=('minmax', 'standard', 'robust')), 'dim_red': CategoricalDistribution(choices=('PCA', None))}, trial_id=26, value=None), 'classification_report': {'0': {'precision': 0.9219858156028369, 'recall': 0.8609271523178808, 'f1-score': 0.8904109589041096, 'support': 151.0}, '1': {'precision': 0.6440677966101694, 'recall': 0.7755102040816326, 'f1-score': 0.7037037037037037, 'support': 49.0}, 'accuracy': 0.84, 'macro avg': {'precision': 0.7830268061065031, 'recall': 0.8182186781997567, 'f1-score': 0.7970573313039067, 'support': 200.0}, 'weighted avg': {'precision': 0.8538959009496334, 'recall': 0.84, 'f1-score': 0.8446676813800101, 'support': 200.0}}}\n",
      "Optimizing model: GradientBoosting | Scoring: f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:20:15,708] Trial 0 finished with value: 0.3442622950819672 and parameters: {'learning_rate': 0.002796081833841858, 'n_estimators': 117, 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 9, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 22}. Best is trial 0 with value: 0.3442622950819672.\n",
      "[I 2024-11-05 09:20:16,780] Trial 1 finished with value: 0.06779661016949153 and parameters: {'learning_rate': 0.030559179538332815, 'n_estimators': 150, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 17, 'max_features': 'sqrt', 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 14}. Best is trial 0 with value: 0.3442622950819672.\n",
      "[I 2024-11-05 09:20:18,235] Trial 2 finished with value: 0.6226415094339622 and parameters: {'learning_rate': 0.06535776864082785, 'n_estimators': 156, 'max_depth': 3, 'min_samples_split': 16, 'min_samples_leaf': 6, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 2 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:20:27,842] Trial 3 finished with value: 0.136986301369863 and parameters: {'learning_rate': 0.013229375684979248, 'n_estimators': 661, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': None, 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 10}. Best is trial 2 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:20:29,547] Trial 4 finished with value: 0.5858585858585859 and parameters: {'learning_rate': 0.001630333398251753, 'n_estimators': 409, 'max_depth': 8, 'min_samples_split': 20, 'min_samples_leaf': 13, 'max_features': 'log2', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 2 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:20:31,954] Trial 5 finished with value: 0.5161290322580645 and parameters: {'learning_rate': 0.008085085794808636, 'n_estimators': 921, 'max_depth': 5, 'min_samples_split': 19, 'min_samples_leaf': 5, 'max_features': 'log2', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 2 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:20:32,980] Trial 6 finished with value: 0.3076923076923077 and parameters: {'learning_rate': 0.056048100719893613, 'n_estimators': 149, 'max_depth': 9, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 'log2', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 7}. Best is trial 2 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:20:36,191] Trial 7 finished with value: 0.4266666666666667 and parameters: {'learning_rate': 0.002727308420512855, 'n_estimators': 966, 'max_depth': 5, 'min_samples_split': 17, 'min_samples_leaf': 4, 'max_features': 'log2', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': None}. Best is trial 2 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:20:37,961] Trial 8 finished with value: 0.43902439024390244 and parameters: {'learning_rate': 0.0922690092578426, 'n_estimators': 306, 'max_depth': 9, 'min_samples_split': 19, 'min_samples_leaf': 11, 'max_features': 'log2', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': None}. Best is trial 2 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:20:39,552] Trial 9 finished with value: 0.40540540540540543 and parameters: {'learning_rate': 0.07583379770096824, 'n_estimators': 491, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 12, 'max_features': 'sqrt', 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': None}. Best is trial 2 with value: 0.6226415094339622.\n",
      "[I 2024-11-05 09:20:42,955] Trial 10 finished with value: 0.6238532110091743 and parameters: {'learning_rate': 0.27627662303081374, 'n_estimators': 678, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 10 with value: 0.6238532110091743.\n",
      "[I 2024-11-05 09:20:46,466] Trial 11 finished with value: 0.6481481481481481 and parameters: {'learning_rate': 0.2738798158513298, 'n_estimators': 731, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 11 with value: 0.6481481481481481.\n",
      "[I 2024-11-05 09:20:50,936] Trial 12 finished with value: 0.6605504587155964 and parameters: {'learning_rate': 0.20533911049640607, 'n_estimators': 729, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:20:53,650] Trial 13 finished with value: 0.6355140186915887 and parameters: {'learning_rate': 0.2528612561951554, 'n_estimators': 797, 'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:20:58,559] Trial 14 finished with value: 0.6355140186915887 and parameters: {'learning_rate': 0.14637189207688728, 'n_estimators': 768, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:02,807] Trial 15 finished with value: 0.5981308411214953 and parameters: {'learning_rate': 0.026735531441695867, 'n_estimators': 594, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:08,199] Trial 16 finished with value: 0.5769230769230769 and parameters: {'learning_rate': 0.15583000518949727, 'n_estimators': 813, 'max_depth': 4, 'min_samples_split': 5, 'min_samples_leaf': 20, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:09,581] Trial 17 finished with value: 0.56 and parameters: {'learning_rate': 0.14352983713418668, 'n_estimators': 530, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 15, 'max_features': 'sqrt', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:24,243] Trial 18 finished with value: 0.4186046511627907 and parameters: {'learning_rate': 0.04071304300696383, 'n_estimators': 876, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 8, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 29}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:27,121] Trial 19 finished with value: 0.5102040816326531 and parameters: {'learning_rate': 0.2991263847244266, 'n_estimators': 700, 'max_depth': 3, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:28,100] Trial 20 finished with value: 0.5631067961165048 and parameters: {'learning_rate': 0.01167232648336645, 'n_estimators': 415, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:31,531] Trial 21 finished with value: 0.6095238095238096 and parameters: {'learning_rate': 0.19927003383457284, 'n_estimators': 782, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:37,226] Trial 22 finished with value: 0.6363636363636364 and parameters: {'learning_rate': 0.10475646223336972, 'n_estimators': 866, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:43,286] Trial 23 finished with value: 0.6346153846153846 and parameters: {'learning_rate': 0.10300106009294348, 'n_estimators': 874, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:48,354] Trial 24 finished with value: 0.6542056074766355 and parameters: {'learning_rate': 0.10632859065081551, 'n_estimators': 987, 'max_depth': 5, 'min_samples_split': 11, 'min_samples_leaf': 5, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:51,426] Trial 25 finished with value: 0.6605504587155964 and parameters: {'learning_rate': 0.1841890357001441, 'n_estimators': 984, 'max_depth': 5, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:54,763] Trial 26 finished with value: 0.3717948717948718 and parameters: {'learning_rate': 0.04175495967766119, 'n_estimators': 994, 'max_depth': 6, 'min_samples_split': 12, 'min_samples_leaf': 6, 'max_features': None, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 3}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:21:58,426] Trial 27 finished with value: 0.6422018348623854 and parameters: {'learning_rate': 0.158997139737455, 'n_estimators': 937, 'max_depth': 5, 'min_samples_split': 14, 'min_samples_leaf': 9, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:22:01,591] Trial 28 finished with value: 0.5376344086021505 and parameters: {'learning_rate': 0.0053052688516004275, 'n_estimators': 609, 'max_depth': 7, 'min_samples_split': 11, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 12 with value: 0.6605504587155964.\n",
      "[I 2024-11-05 09:22:10,914] Trial 29 finished with value: 0.4316546762589928 and parameters: {'learning_rate': 0.025256186806323907, 'n_estimators': 862, 'max_depth': 6, 'min_samples_split': 14, 'min_samples_leaf': 10, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 23}. Best is trial 12 with value: 0.6605504587155964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_name: GradientBoosting | study.best_trial.value: 0.6605504587155964\n",
      "Current Model: GradientBoosting, Current Score: 0.6605504587155964 | Best Model: DecisionTree, Best Score: 0.7079646017699115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:22:15,629] A new study created in memory with name: no-name-d2c12bdb-ba5d-47ff-a6a4-cbc8a266e2ad\n",
      "[I 2024-11-05 09:22:15,630] Trial 0 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized GradientBoosting pipeline and test metrics to GradientBoosting_pipeline.pkl\n",
      "Best Parameters: {'classifier_name': 'GradientBoosting', 'f1': 0.6605504587155964, 'accuracy': 0.815, 'precision': 0.6, 'recall': 0.7346938775510204, 'roc_auc': 0.8132180024327611, 'score_training_f1': 0.5540994500670686, 'best_params': FrozenTrial(number=12, state=1, values=[0.6605504587155964], datetime_start=datetime.datetime(2024, 11, 5, 9, 20, 46, 466881), datetime_complete=datetime.datetime(2024, 11, 5, 9, 20, 50, 936103), params={'learning_rate': 0.20533911049640607, 'n_estimators': 729, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.3, log=True, low=0.001, step=None), 'n_estimators': IntDistribution(high=1000, log=False, low=100, step=1), 'max_depth': IntDistribution(high=10, log=False, low=3, step=1), 'min_samples_split': IntDistribution(high=20, log=False, low=2, step=1), 'min_samples_leaf': IntDistribution(high=20, log=False, low=1, step=1), 'max_features': CategoricalDistribution(choices=('sqrt', 'log2', None)), 'categorical_strategy': CategoricalDistribution(choices=('most_frequent', 'constant')), 'numerical_strategy': CategoricalDistribution(choices=('mean', 'median', 'most_frequent')), 'outlier_strategy': CategoricalDistribution(choices=('power_transform', 'log_transform', 'iqr_clip', 'iqr_median', 'iqr_mean')), 'resampler': CategoricalDistribution(choices=('RandomOverSampler', 'SMOTEENN', 'SMOTETomek')), 'scaler': CategoricalDistribution(choices=('minmax', 'standard', 'robust')), 'dim_red': CategoricalDistribution(choices=('PCA', None))}, trial_id=12, value=None), 'classification_report': {'0': {'precision': 0.9071428571428571, 'recall': 0.8410596026490066, 'f1-score': 0.872852233676976, 'support': 151.0}, '1': {'precision': 0.6, 'recall': 0.7346938775510204, 'f1-score': 0.6605504587155964, 'support': 49.0}, 'accuracy': 0.815, 'macro avg': {'precision': 0.7535714285714286, 'recall': 0.7878767401000135, 'f1-score': 0.7667013461962862, 'support': 200.0}, 'weighted avg': {'precision': 0.8318928571428572, 'recall': 0.815, 'f1-score': 0.820838298811438, 'support': 200.0}}}\n",
      "Optimizing model: LogisticRegression | Scoring: f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.06397e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.78474e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=8.39834e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=8.7336e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.29608e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.26712e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.49339e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.68717e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.71557e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.29651e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.02882e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "[I 2024-11-05 09:22:15,910] Trial 1 finished with value: 0.6140350877192983 and parameters: {'solver': 'newton-cholesky', 'max_iter': 14669, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "[I 2024-11-05 09:22:16,060] Trial 2 finished with value: 0.5148514851485149 and parameters: {'solver': 'lbfgs', 'max_iter': 21610, 'penalty': 'l2', 'C': 0.01019152105281956, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 29}. Best is trial 1 with value: 0.6140350877192983.\n",
      "[I 2024-11-05 09:22:16,062] Trial 3 pruned. \n",
      "[I 2024-11-05 09:22:16,064] Trial 4 pruned. \n",
      "[I 2024-11-05 09:22:16,070] Trial 5 pruned. \n",
      "[I 2024-11-05 09:22:16,072] Trial 6 pruned. \n",
      "[I 2024-11-05 09:22:16,310] Trial 7 finished with value: 0.4266666666666667 and parameters: {'solver': 'sag', 'max_iter': 21442, 'penalty': 'l2', 'C': 0.0002933949171403748, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 8}. Best is trial 1 with value: 0.6140350877192983.\n",
      "[I 2024-11-05 09:22:16,525] Trial 8 finished with value: 0.5736434108527132 and parameters: {'solver': 'newton-cholesky', 'max_iter': 47185, 'penalty': 'l2', 'C': 0.0015887462800160334, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "[I 2024-11-05 09:22:16,527] Trial 9 pruned. \n",
      "[I 2024-11-05 09:22:16,706] Trial 10 finished with value: 0.39357429718875503 and parameters: {'solver': 'saga', 'max_iter': 39070, 'penalty': 'l1', 'C': 5.476827096420728e-10, 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.54672e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.40454e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=5.30014e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.10235e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=7.36626e-19): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.34842e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.00577e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=5.70941e-19): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.2637e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.9717e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.08443e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "[I 2024-11-05 09:22:16,954] Trial 11 finished with value: 0.6071428571428571 and parameters: {'solver': 'newton-cholesky', 'max_iter': 49749, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.40454e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.10235e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.54672e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=7.36626e-19): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=5.30014e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.34842e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.00577e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.2637e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=5.70941e-19): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.9717e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.08443e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "[I 2024-11-05 09:22:17,164] Trial 12 finished with value: 0.6071428571428571 and parameters: {'solver': 'newton-cholesky', 'max_iter': 49476, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.00706e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.99689e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.29535e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.51146e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=8.22281e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.01255e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.84075e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.79558e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=6.6148e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.20819e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.9517e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "[I 2024-11-05 09:22:17,388] Trial 13 finished with value: 0.6140350877192983 and parameters: {'solver': 'newton-cholesky', 'max_iter': 38106, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=5.48544e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.47491e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.00579e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.78949e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.54545e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=5.79638e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.47128e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=6.07552e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.08351e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.2259e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=9.83149e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "[I 2024-11-05 09:22:17,603] Trial 14 finished with value: 0.6 and parameters: {'solver': 'newton-cholesky', 'max_iter': 35238, 'penalty': None, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "[I 2024-11-05 09:22:18,460] Trial 15 finished with value: 0.5669291338582677 and parameters: {'solver': 'saga', 'max_iter': 38520, 'penalty': 'l1', 'C': 330.2416241146025, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "[I 2024-11-05 09:22:19,446] Trial 16 finished with value: 0.5714285714285714 and parameters: {'solver': 'lbfgs', 'max_iter': 34586, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTETomek', 'scaler': 'robust', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "[I 2024-11-05 09:22:19,602] Trial 17 finished with value: 0.4444444444444444 and parameters: {'solver': 'newton-cholesky', 'max_iter': 41548, 'penalty': None, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 23}. Best is trial 1 with value: 0.6140350877192983.\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=8.39834e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.06397e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.78474e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.29608e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=8.7336e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.26712e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.49339e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.29651e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.68717e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.71557e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/linear_model/_glm/_newton_solver.py:497: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.02882e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "[I 2024-11-05 09:22:19,746] Trial 18 finished with value: 0.6140350877192983 and parameters: {'solver': 'newton-cholesky', 'max_iter': 33925, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.6140350877192983.\n",
      "[I 2024-11-05 09:22:19,758] Trial 19 pruned. \n",
      "[I 2024-11-05 09:22:19,976] Trial 20 finished with value: 0.6260869565217392 and parameters: {'solver': 'saga', 'max_iter': 42775, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 20 with value: 0.6260869565217392.\n",
      "[I 2024-11-05 09:22:20,190] Trial 21 finished with value: 0.6260869565217392 and parameters: {'solver': 'saga', 'max_iter': 44447, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 20 with value: 0.6260869565217392.\n",
      "[I 2024-11-05 09:22:20,478] Trial 22 finished with value: 0.6260869565217392 and parameters: {'solver': 'saga', 'max_iter': 44123, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 20 with value: 0.6260869565217392.\n",
      "[I 2024-11-05 09:22:20,753] Trial 23 finished with value: 0.6260869565217392 and parameters: {'solver': 'saga', 'max_iter': 44456, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 20 with value: 0.6260869565217392.\n",
      "[I 2024-11-05 09:22:21,172] Trial 24 finished with value: 0.6086956521739131 and parameters: {'solver': 'saga', 'max_iter': 44000, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 20 with value: 0.6260869565217392.\n",
      "[I 2024-11-05 09:22:21,464] Trial 25 finished with value: 0.33663366336633666 and parameters: {'solver': 'saga', 'max_iter': 44168, 'penalty': None, 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 2}. Best is trial 20 with value: 0.6260869565217392.\n",
      "[I 2024-11-05 09:22:21,664] Trial 26 finished with value: 0.6260869565217392 and parameters: {'solver': 'saga', 'max_iter': 46400, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 20 with value: 0.6260869565217392.\n",
      "[I 2024-11-05 09:22:21,906] Trial 27 finished with value: 0.0 and parameters: {'solver': 'saga', 'max_iter': 41024, 'penalty': 'l2', 'C': 5.236845426480043e-07, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 20 with value: 0.6260869565217392.\n",
      "[I 2024-11-05 09:22:22,471] Trial 28 finished with value: 0.6140350877192983 and parameters: {'solver': 'saga', 'max_iter': 32329, 'penalty': 'l1', 'C': 2.0718174356525507, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}. Best is trial 20 with value: 0.6260869565217392.\n",
      "[I 2024-11-05 09:22:22,728] Trial 29 finished with value: 0.39357429718875503 and parameters: {'solver': 'saga', 'max_iter': 41429, 'penalty': 'elasticnet', 'C': 1.8119702344393877e-05, 'l1_ratio': 0.9404878333411633, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 14}. Best is trial 20 with value: 0.6260869565217392.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_name: LogisticRegression | study.best_trial.value: 0.6260869565217392\n",
      "Current Model: LogisticRegression, Current Score: 0.6260869565217392 | Best Model: DecisionTree, Best Score: 0.7079646017699115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:22:23,020] A new study created in memory with name: no-name-e937a4be-bdf5-4061-a479-e79e3dd6aa15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized LogisticRegression pipeline and test metrics to LogisticRegression_pipeline.pkl\n",
      "Best Parameters: {'classifier_name': 'LogisticRegression', 'f1': 0.6260869565217392, 'accuracy': 0.785, 'precision': 0.5454545454545454, 'recall': 0.7346938775510204, 'roc_auc': 0.8003784295175024, 'score_training_f1': 0.5567047839572756, 'best_params': FrozenTrial(number=20, state=1, values=[0.6260869565217392], datetime_start=datetime.datetime(2024, 11, 5, 9, 22, 19, 758801), datetime_complete=datetime.datetime(2024, 11, 5, 9, 22, 19, 976106), params={'solver': 'saga', 'max_iter': 42775, 'penalty': None, 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': None}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'solver': CategoricalDistribution(choices=('newton-cholesky', 'lbfgs', 'liblinear', 'sag', 'saga')), 'max_iter': IntDistribution(high=50000, log=False, low=10000, step=1), 'penalty': CategoricalDistribution(choices=('l1', 'l2', 'elasticnet', None)), 'categorical_strategy': CategoricalDistribution(choices=('most_frequent', 'constant')), 'numerical_strategy': CategoricalDistribution(choices=('mean', 'median', 'most_frequent')), 'outlier_strategy': CategoricalDistribution(choices=('power_transform', 'log_transform', 'iqr_clip', 'iqr_median', 'iqr_mean')), 'resampler': CategoricalDistribution(choices=('RandomOverSampler', 'SMOTEENN', 'SMOTETomek')), 'scaler': CategoricalDistribution(choices=('minmax', 'standard', 'robust')), 'dim_red': CategoricalDistribution(choices=('PCA', None))}, trial_id=20, value=None), 'classification_report': {'0': {'precision': 0.9029850746268657, 'recall': 0.8013245033112583, 'f1-score': 0.8491228070175438, 'support': 151.0}, '1': {'precision': 0.5454545454545454, 'recall': 0.7346938775510204, 'f1-score': 0.6260869565217392, 'support': 49.0}, 'accuracy': 0.785, 'macro avg': {'precision': 0.7242198100407056, 'recall': 0.7680091904311394, 'f1-score': 0.7376048817696415, 'support': 200.0}, 'weighted avg': {'precision': 0.8153900949796473, 'recall': 0.785, 'f1-score': 0.7944790236460716, 'support': 200.0}}}\n",
      "Optimizing model: KNeighbors | Scoring: f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2024-11-05 09:22:23,249] Trial 0 finished with value: 0.0 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'p': 2, 'leaf_size': 48, 'metric': 'chebyshev', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 0 with value: 0.0.\n",
      "[I 2024-11-05 09:22:23,418] Trial 1 finished with value: 0.38009049773755654 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'p': 1, 'leaf_size': 87, 'metric': 'chebyshev', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': None}. Best is trial 1 with value: 0.38009049773755654.\n",
      "[I 2024-11-05 09:22:23,578] Trial 2 finished with value: 0.32653061224489793 and parameters: {'n_neighbors': 37, 'weights': 'uniform', 'p': 2, 'leaf_size': 37, 'metric': 'minkowski', 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 20}. Best is trial 1 with value: 0.38009049773755654.\n",
      "[I 2024-11-05 09:22:23,876] Trial 3 finished with value: 0.4479166666666667 and parameters: {'n_neighbors': 37, 'weights': 'uniform', 'p': 1, 'leaf_size': 82, 'metric': 'minkowski', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 3 with value: 0.4479166666666667.\n",
      "[I 2024-11-05 09:22:24,012] Trial 4 finished with value: 0.23529411764705882 and parameters: {'n_neighbors': 36, 'weights': 'uniform', 'p': 1, 'leaf_size': 69, 'metric': 'chebyshev', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 28}. Best is trial 3 with value: 0.4479166666666667.\n",
      "[I 2024-11-05 09:22:24,272] Trial 5 finished with value: 0.3087248322147651 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'p': 2, 'leaf_size': 94, 'metric': 'minkowski', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 3}. Best is trial 3 with value: 0.4479166666666667.\n",
      "[I 2024-11-05 09:22:24,442] Trial 6 finished with value: 0.4968152866242038 and parameters: {'n_neighbors': 49, 'weights': 'uniform', 'p': 1, 'leaf_size': 93, 'metric': 'minkowski', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:24,605] Trial 7 finished with value: 0.4100418410041841 and parameters: {'n_neighbors': 34, 'weights': 'distance', 'p': 2, 'leaf_size': 39, 'metric': 'minkowski', 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:24,776] Trial 8 finished with value: 0.36809815950920244 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'p': 1, 'leaf_size': 53, 'metric': 'chebyshev', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 11}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:25,026] Trial 9 finished with value: 0.358974358974359 and parameters: {'n_neighbors': 33, 'weights': 'distance', 'p': 1, 'leaf_size': 39, 'metric': 'euclidean', 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 11}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:25,277] Trial 10 finished with value: 0.46808510638297873 and parameters: {'n_neighbors': 50, 'weights': 'uniform', 'p': 1, 'leaf_size': 15, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:25,420] Trial 11 finished with value: 0.46808510638297873 and parameters: {'n_neighbors': 50, 'weights': 'uniform', 'p': 1, 'leaf_size': 18, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:25,563] Trial 12 finished with value: 0.4968152866242038 and parameters: {'n_neighbors': 49, 'weights': 'uniform', 'p': 1, 'leaf_size': 11, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:25,687] Trial 13 finished with value: 0.4583333333333333 and parameters: {'n_neighbors': 44, 'weights': 'uniform', 'p': 1, 'leaf_size': 73, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:25,808] Trial 14 finished with value: 0.35294117647058826 and parameters: {'n_neighbors': 1, 'weights': 'uniform', 'p': 1, 'leaf_size': 100, 'metric': 'euclidean', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:25,957] Trial 15 finished with value: 0.4657534246575342 and parameters: {'n_neighbors': 42, 'weights': 'uniform', 'p': 1, 'leaf_size': 63, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:26,141] Trial 16 finished with value: 0.4217687074829932 and parameters: {'n_neighbors': 23, 'weights': 'uniform', 'p': 1, 'leaf_size': 27, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:26,259] Trial 17 finished with value: 0.43243243243243246 and parameters: {'n_neighbors': 43, 'weights': 'uniform', 'p': 2, 'leaf_size': 78, 'metric': 'minkowski', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:26,416] Trial 18 finished with value: 0.4897959183673469 and parameters: {'n_neighbors': 45, 'weights': 'uniform', 'p': 1, 'leaf_size': 26, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:26,588] Trial 19 finished with value: 0.41350210970464135 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'p': 1, 'leaf_size': 60, 'metric': 'euclidean', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:26,723] Trial 20 finished with value: 0.09375 and parameters: {'n_neighbors': 50, 'weights': 'uniform', 'p': 2, 'leaf_size': 12, 'metric': 'minkowski', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'log_transform', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:26,878] Trial 21 finished with value: 0.4897959183673469 and parameters: {'n_neighbors': 45, 'weights': 'uniform', 'p': 1, 'leaf_size': 27, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:27,021] Trial 22 finished with value: 0.4246575342465753 and parameters: {'n_neighbors': 41, 'weights': 'uniform', 'p': 1, 'leaf_size': 27, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:27,169] Trial 23 finished with value: 0.47368421052631576 and parameters: {'n_neighbors': 47, 'weights': 'uniform', 'p': 1, 'leaf_size': 10, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:27,308] Trial 24 finished with value: 0.4246575342465753 and parameters: {'n_neighbors': 41, 'weights': 'uniform', 'p': 1, 'leaf_size': 20, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:27,460] Trial 25 finished with value: 0.4697986577181208 and parameters: {'n_neighbors': 47, 'weights': 'uniform', 'p': 1, 'leaf_size': 49, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:27,624] Trial 26 finished with value: 0.4864864864864865 and parameters: {'n_neighbors': 39, 'weights': 'distance', 'p': 1, 'leaf_size': 32, 'metric': 'minkowski', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 30}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:27,842] Trial 27 finished with value: 0.48717948717948717 and parameters: {'n_neighbors': 47, 'weights': 'uniform', 'p': 1, 'leaf_size': 21, 'metric': 'euclidean', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_mean', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:28,057] Trial 28 finished with value: 0.4114285714285714 and parameters: {'n_neighbors': 31, 'weights': 'uniform', 'p': 1, 'leaf_size': 45, 'metric': 'manhattan', 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2024-11-05 09:22:28,205] Trial 29 finished with value: 0.0 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'p': 2, 'leaf_size': 59, 'metric': 'chebyshev', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 6 with value: 0.4968152866242038.\n",
      "[I 2024-11-05 09:22:28,347] A new study created in memory with name: no-name-3705cfa9-7e64-4427-8590-18fd2e30fe64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_name: KNeighbors | study.best_trial.value: 0.4968152866242038\n",
      "Current Model: KNeighbors, Current Score: 0.4968152866242038 | Best Model: DecisionTree, Best Score: 0.7079646017699115\n",
      "Serialized KNeighbors pipeline and test metrics to KNeighbors_pipeline.pkl\n",
      "Best Parameters: {'classifier_name': 'KNeighbors', 'f1': 0.4968152866242038, 'accuracy': 0.605, 'precision': 0.3611111111111111, 'recall': 0.7959183673469388, 'roc_auc': 0.7269901338018651, 'score_training_f1': 0.47193291044710495, 'best_params': FrozenTrial(number=6, state=1, values=[0.4968152866242038], datetime_start=datetime.datetime(2024, 11, 5, 9, 22, 24, 272936), datetime_complete=datetime.datetime(2024, 11, 5, 9, 22, 24, 442725), params={'n_neighbors': 49, 'weights': 'uniform', 'p': 1, 'leaf_size': 93, 'metric': 'minkowski', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_neighbors': IntDistribution(high=50, log=False, low=1, step=1), 'weights': CategoricalDistribution(choices=('uniform', 'distance')), 'p': IntDistribution(high=2, log=False, low=1, step=1), 'leaf_size': IntDistribution(high=100, log=False, low=10, step=1), 'metric': CategoricalDistribution(choices=('euclidean', 'manhattan', 'minkowski', 'chebyshev')), 'categorical_strategy': CategoricalDistribution(choices=('most_frequent', 'constant')), 'numerical_strategy': CategoricalDistribution(choices=('mean', 'median', 'most_frequent')), 'outlier_strategy': CategoricalDistribution(choices=('power_transform', 'log_transform', 'iqr_clip', 'iqr_median', 'iqr_mean')), 'resampler': CategoricalDistribution(choices=('RandomOverSampler', 'SMOTEENN', 'SMOTETomek')), 'scaler': CategoricalDistribution(choices=('minmax', 'standard', 'robust')), 'dim_red': CategoricalDistribution(choices=('PCA', None))}, trial_id=6, value=None), 'classification_report': {'0': {'precision': 0.8913043478260869, 'recall': 0.543046357615894, 'f1-score': 0.6748971193415638, 'support': 151.0}, '1': {'precision': 0.3611111111111111, 'recall': 0.7959183673469388, 'f1-score': 0.4968152866242038, 'support': 49.0}, 'accuracy': 0.605, 'macro avg': {'precision': 0.626207729468599, 'recall': 0.6694823624814163, 'f1-score': 0.5858562029828838, 'support': 200.0}, 'weighted avg': {'precision': 0.7614070048309179, 'recall': 0.605, 'f1-score': 0.6312670703258106, 'support': 200.0}}}\n",
      "Optimizing model: CatBoost | Scoring: f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-05 09:22:36,064] Trial 0 finished with value: 0.5679012345679012 and parameters: {'objective': 'Logloss', 'colsample_bylevel': 0.05168966738485102, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTETomek', 'scaler': 'minmax', 'dim_red': None}. Best is trial 0 with value: 0.5679012345679012.\n",
      "[I 2024-11-05 09:22:44,645] Trial 1 finished with value: 0.49411764705882355 and parameters: {'objective': 'Logloss', 'colsample_bylevel': 0.05318175534934874, 'depth': 5, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': None}. Best is trial 0 with value: 0.5679012345679012.\n",
      "/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2024-11-05 09:22:45,238] Trial 2 finished with value: 0.0 and parameters: {'objective': 'Logloss', 'colsample_bylevel': 0.011865968099305419, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'log_transform', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': 'PCA', 'pca_n_components': 20}. Best is trial 0 with value: 0.5679012345679012.\n",
      "[I 2024-11-05 09:22:46,287] Trial 3 finished with value: 0.5785123966942148 and parameters: {'objective': 'Logloss', 'colsample_bylevel': 0.0430855862560885, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTEENN', 'scaler': 'minmax', 'dim_red': None}. Best is trial 3 with value: 0.5785123966942148.\n",
      "[I 2024-11-05 09:22:52,508] Trial 4 finished with value: 0.28205128205128205 and parameters: {'objective': 'CrossEntropy', 'colsample_bylevel': 0.06627023560641883, 'depth': 4, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 28}. Best is trial 3 with value: 0.5785123966942148.\n",
      "[I 2024-11-05 09:22:57,197] Trial 5 finished with value: 0.41081081081081083 and parameters: {'objective': 'CrossEntropy', 'colsample_bylevel': 0.09639663826064336, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'power_transform', 'resampler': 'RandomOverSampler', 'scaler': 'robust', 'dim_red': 'PCA', 'pca_n_components': 3}. Best is trial 3 with value: 0.5785123966942148.\n",
      "[I 2024-11-05 09:23:09,330] Trial 6 finished with value: 0.5714285714285714 and parameters: {'objective': 'Logloss', 'colsample_bylevel': 0.05087780201143927, 'depth': 6, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'categorical_strategy': 'constant', 'numerical_strategy': 'mean', 'outlier_strategy': 'iqr_clip', 'resampler': 'RandomOverSampler', 'scaler': 'minmax', 'dim_red': None}. Best is trial 3 with value: 0.5785123966942148.\n",
      "[I 2024-11-05 09:25:39,004] Trial 7 finished with value: 0.594059405940594 and parameters: {'objective': 'Logloss', 'colsample_bylevel': 0.0776680452246569, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'power_transform', 'resampler': 'SMOTEENN', 'scaler': 'standard', 'dim_red': None}. Best is trial 7 with value: 0.594059405940594.\n",
      "[I 2024-11-05 09:25:42,258] Trial 8 finished with value: 0.4125 and parameters: {'objective': 'CrossEntropy', 'colsample_bylevel': 0.030127463240661352, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'categorical_strategy': 'most_frequent', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_clip', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 11}. Best is trial 7 with value: 0.594059405940594.\n",
      "[I 2024-11-05 09:25:42,987] Trial 9 finished with value: 0.4171779141104294 and parameters: {'objective': 'Logloss', 'colsample_bylevel': 0.041157614255993236, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'categorical_strategy': 'constant', 'numerical_strategy': 'most_frequent', 'outlier_strategy': 'iqr_median', 'resampler': 'SMOTETomek', 'scaler': 'standard', 'dim_red': 'PCA', 'pca_n_components': 11}. Best is trial 7 with value: 0.594059405940594.\n",
      "[I 2024-11-05 09:29:39,479] Trial 10 finished with value: 0.594059405940594 and parameters: {'objective': 'CrossEntropy', 'colsample_bylevel': 0.0783801515591312, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None}. Best is trial 7 with value: 0.594059405940594.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config_path = \"model_config.yaml\"\n",
    "    best_model, best_params, current_score = run_optimization(config_path, n_trials=30, scoring='f1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to deserialise a pipeline for inference\n",
    "def load_pipeline(model_name):\n",
    "    return joblib.load(f'{model_name}_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the serialized model and metrics test results\n",
    "pipeline, metrics_test = load_pipeline(\"LogisticRegression\")\n",
    "# Perform predictions\n",
    "#predictions = pipeline.predict(new_data)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables from the file\n",
    "with open(\"fitted_best_model_and_params.pkl\", \"rb\") as f:\n",
    "    model_name, pipeline_fitted = joblib.load(f)\n",
    "\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"pipeline_fitted: {pipeline_fitted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fitted.fit(X_train, y_train)\n",
    "y_pred = pipeline_fitted.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "y_pred_proba = pipeline_fitted.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba) #Calculate Roc\n",
    "\n",
    "test_results = {\n",
    "    'f1_score': f1,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'roc_auc': roc_auc,\n",
    "    'model_name': model_name,\n",
    "    'best_params': best_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best_model: {best_model}\")\n",
    "print(f\"best_params: {best_params}\")\n",
    "\n",
    "# Create an instance of the ModelFactory class with best_model and best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables from the file\n",
    "with open(\"best_model_and_params.pkl\", \"rb\") as f:\n",
    "    best_model, best_params = joblib.load(f)\n",
    "\n",
    "print(f\"best_model: {best_model}\")\n",
    "print(f\"best_params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_pipeline_model_and_params(best_model, trial=None, model_mode='train', input_params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba) #Calculate Roc\n",
    "\n",
    "test_results = {\n",
    "    'f1_score': f1,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'roc_auc': roc_auc,\n",
    "    'best_model': best_model,\n",
    "    'best_params': best_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `pipeline` is your final fitted pipeline\n",
    "# Also, assume `X_train` is your training DataFrame\n",
    "\n",
    "# Fit the pipeline (if not already fitted)\n",
    "#pipeline.fit(X_train)\n",
    "\n",
    "# If you have a ColumnTransformer, find it in the pipeline\n",
    "column_transformer = None\n",
    "for name, step in pipeline.named_steps.items():\n",
    "    if isinstance(step, ColumnTransformer):\n",
    "        column_transformer = step\n",
    "        break\n",
    "\n",
    "# If you have a ColumnTransformer, get the feature names\n",
    "if column_transformer is not None:\n",
    "    # Get transformed column names\n",
    "    feature_names = []\n",
    "    for name, transformer, columns in column_transformer.transformers_:\n",
    "        if transformer != 'drop':\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                feature_names.extend(transformer.get_feature_names_out(columns))\n",
    "            else:\n",
    "                # If the transformer does not have get_feature_names_out method\n",
    "                feature_names.extend(columns)\n",
    "else:\n",
    "    # No ColumnTransformer, fallback to input features\n",
    "    feature_names = X_train.columns.tolist()\n",
    "\n",
    "print(\"Extracted Feature Names: \", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_feature_importance(model, X, y, feature_names, n_top=10):\n",
    "    \"\"\"\n",
    "    This function takes in a dictionary of models, the dataset X, y, and the feature names.\n",
    "    It fits each model, extracts feature importances (if available), \n",
    "    and plots the top n features.\n",
    "\n",
    "    Parameters:\n",
    "    models (dict): A dictionary containing model names and their respective model objects.\n",
    "    X (np.ndarray): Feature dataset.\n",
    "    y (pd.Series): Target variable.\n",
    "    feature_names (list): List of feature names after transformations.\n",
    "    n_top (int): Number of top features to display. Default is 10.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    model_name = 'classifier'\n",
    "     \n",
    "    # Fit the model\n",
    "    #model.fit(X, y)\n",
    "    print(f\"Feature ranking for model: {model_name}\")\n",
    "    try:\n",
    "        # Check if the model has the attribute `feature_importances_`\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # Get feature importances\n",
    "            importance_scores = model.feature_importances_\n",
    "            \n",
    "        else:                 \n",
    "            print(f\"{model_name} does not support feature importances.\")\n",
    "            importance_scores = model.coef_[0]\n",
    "            \n",
    "        # Create a DataFrame from feature names and importances\n",
    "        data = {'Feature': feature_names, 'Score': importance_scores}\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        \n",
    "        # Take the absolute value of the score\n",
    "        df['Abs_Score'] = np.abs(df['Score'])\n",
    "        \n",
    "        df_sorted = df.sort_values(by=\"Abs_Score\", ascending=False)\n",
    "        if n_top:\n",
    "            # Sort by absolute value of score in descending order (top 10)\n",
    "            df_sorted = df_sorted.head(n_top)\n",
    "        \n",
    "        # Define a color palette based on score values (positive = green, negative = red)\n",
    "        colors = [\"green\" if score > 0 else \"red\" for score in df_sorted[\"Score\"]]\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Create the bar chart with Seaborn\n",
    "        sns.barplot(x=\"Feature\", y=\"Score\", hue=\"Feature\", legend=False, data=df_sorted, palette=colors)\n",
    "        \n",
    "        # Customize the plot for better visual appeal\n",
    "        plt.xlabel(\"Feature\")\n",
    "        plt.ylabel(\"Feature Importance Score\")\n",
    "        plt.title(f\"Feature Importance in {model_name} Classification\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
    "        plt.tight_layout()  # Adjust spacing between elements\n",
    "    \n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "                \n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while extracting feature importances: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = pipeline.named_steps['model']\n",
    "fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the display feature importance function after model evaluation\n",
    "display_feature_importance(fitted_model, X_train, y_train, feature_names, n_top=10)  # Specify how many top features to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_feature_importance(model, feature_names=None, top_n=20):\n",
    "    \"\"\"\n",
    "    Plot the feature importance of a fitted tree-based model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator\n",
    "        A fitted tree-based model with `feature_importances_` attribute (e.g., RandomForest, GradientBoosting).\n",
    "    \n",
    "    feature_names : list or None\n",
    "        List of feature names. If None, numerical indices are used as feature names.\n",
    "    \n",
    "    top_n : int, default=20\n",
    "        The number of top features to plot. If `None`, all features will be plotted.\n",
    "    \"\"\"\n",
    "    # Check if the model has feature_importances_ attribute\n",
    "    try:\n",
    "        if not hasattr(model, 'feature_importances_'):\n",
    "            importance_values = model.coef_[0]\n",
    "        else:\n",
    "            # Get feature importance values and sort them in descending order\n",
    "            importance_values = model.feature_importances_\n",
    "    except:\n",
    "        raise ValueError(\"The model does not have `feature_importances_` attribute.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create a DataFrame for better manipulation and sorting\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature {i}' for i in range(len(importance_values))]\n",
    "    \n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance_values})\n",
    "    \n",
    "    # Sort features by importance\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # If top_n is specified, select the top_n features\n",
    "    if top_n is not None and top_n < len(importance_df):\n",
    "        importance_df = importance_df.head(top_n)\n",
    "    \n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df, hue='Feature', palette='viridis', dodge=False, legend=False)\n",
    "    plt.title('Top Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = pipeline.named_steps['model']\n",
    "fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to plot the feature importance\n",
    "plot_feature_importance(fitted_model, feature_names=feature_names, top_n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311_insure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
