{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import yaml\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import Any, Dict, Tuple\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, StandardScaler, OrdinalEncoder, PowerTransformer, RobustScaler, MinMaxScaler,\n",
    "    FunctionTransformer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import  accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "from utils_machine_learning import rename_columns_to_snake_case\n",
    "\n",
    "from custom_transformers import (\n",
    "    DropRedundantColumns,\n",
    "    CreateNewFeature,\n",
    "\n",
    "    LogTransformer,\n",
    "    PowerTransformerWrapper,\n",
    "    OutlierTransformer,\n",
    "    OutlierDetector,\n",
    "    ReplaceValueTransformer,\n",
    "    OutlierHandler,\n",
    "\n",
    ")\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import joblib\n",
    "from typing import Union\n",
    "\n",
    "from imblearn.over_sampling import (\n",
    "    RandomOverSampler,\n",
    "    ADASYN,\n",
    "    \n",
    ")\n",
    "from imblearn.under_sampling import (\n",
    "    RandomUnderSampler,\n",
    "    NearMiss,\n",
    ")\n",
    "from imblearn.combine import (\n",
    "    SMOTEENN,\n",
    "    SMOTETomek\n",
    ")\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from the CSV file and return it as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset loaded from the CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = 'https://github.com/donadviser/datasets/raw/master/data-don/auto_insurance_claim_fraud.csv'\n",
    "    data = pd.read_csv(data_path, sep=\",\")\n",
    "    return (data\n",
    "            .pipe(rename_columns_to_snake_case)\n",
    "            #.dropna()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = load_dataset()\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_features = ['policy_state', 'collision_type', 'property_damage', 'police_report_available', \n",
    "                  'insured_sex', 'insured_education_level', 'insured_relationship', 'incident_type', \n",
    "                  'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', \n",
    "                  'policy_deductable', 'number_of_vehicles_involved', 'bodily_injuries', 'witnesses', \n",
    "                  'incident_period_of_day']\n",
    "\n",
    "numerical_features = ['months_as_customer',  'age', 'policy_annual_premium', 'injury_claim', \n",
    "                      'property_claim', 'vehicle_claim', 'vehicle_age','total_claim_amount']\n",
    "\n",
    "ordinal_features = ['insured_occupation', 'insured_hobbies', 'auto_make']\n",
    "\n",
    "transform_features = ['umbrella_limit', 'capital_gains', 'capital_loss']\n",
    "\n",
    "drop_columns = ['policy_number','policy_bind_date','policy_csl', 'insured_zip','incident_date',\n",
    "                'incident_location','auto_model','auto_year', 'incident_hour_of_the_day',\n",
    "                ]\n",
    "\n",
    "bins_hour = [0, 6, 11, 16, 21, 24]  # Time bins for different periods of the day\n",
    "names_period = [\"early_morning\", \"morning\", \"afternoon\", \"evening\", \"night\"] \n",
    "\n",
    "target_col = 'fraud_reported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns= ['policy_state', 'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', 'property_damage', 'police_report_available', 'auto_make', 'policy_deductable', 'number_of_vehicles_involved', 'bodily_injuries', 'witnesses', 'fraud_reported', 'incident_hour_of_the_day']\n",
    "numerical_columns =['months_as_customer', 'age', 'policy_annual_premium', 'umbrella_limit', 'capital_gains', 'capital_loss', 'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in numerical_columns:\n",
    "    print(f'Min - Max values for numerical Column: {item}')\n",
    "    print(f\"{data_raw[item].min()} - {data_raw[item].max()}\")\n",
    "    print('----------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset is loaded in a pandas dataframe\n",
    "X, y = data_raw.drop(columns=[target_col]), data_raw[target_col]\n",
    "\n",
    "# Splitting the dataset\n",
    "y = y.map({'Y': 1, 'N': 0})  # Map target labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML configuration file\n",
    "def load_yaml_config(config_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the YAML configuration file containing model and hyperparameter definitions.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the YAML configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The loaded configuration as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    \"\"\"\n",
    "    HyperparameterTuner to return hyperparameters for each classifier.\n",
    "    \"\"\"\n",
    "    def get_params(self, trial: optuna.Trial, classifier_name: str):\n",
    "        if classifier_name == \"RandomForest\":\n",
    "            return {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "            }\n",
    "        elif classifier_name == \"DecisionTree\":\n",
    "            return {\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "            }\n",
    "        elif classifier_name == \"LGBM\":\n",
    "            return {\n",
    "                \"objective\": \"binary\",\n",
    "                \"metric\": \"binary_logloss\",\n",
    "                \"verbosity\": -1,\n",
    "                \"boosting_type\": \"gbdt\",\n",
    "                \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "                \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "                \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "                \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            }\n",
    "        elif classifier_name == \"XGBoost\":\n",
    "            return {\n",
    "                \"verbosity\": 0,\n",
    "                \"objective\": \"binary:logistic\",\n",
    "                \"eval_metric\": \"auc\",\n",
    "                \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "                \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "                \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "            }\n",
    "        elif classifier_name == \"CatBoost\":\n",
    "            return {\n",
    "                \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "                \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "                \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "                \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "                \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n",
    "            }\n",
    "        elif classifier_name == \"LogisticRegression\":\n",
    "            # Basic hyperparameters\n",
    "            params = {\n",
    "                \"solver\": trial.suggest_categorical('solver', ['newton-cholesky', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "                \"max_iter\": trial.suggest_int('max_iter', 10000, 50000),  # Increased max_iter to allow for better convergence\n",
    "            }\n",
    "\n",
    "            # Suggest penalty from a unified set\n",
    "            all_penalties = ['l1', 'l2', 'elasticnet', None]  # Unified penalties\n",
    "            params['penalty'] = trial.suggest_categorical('penalty', all_penalties)\n",
    "\n",
    "            # Only suggest C if penalty is not None\n",
    "            if params['penalty'] is not None:\n",
    "                params[\"C\"] = trial.suggest_float('C', 1e-10, 1000, log=True)\n",
    "            \n",
    "            # Only suggest l1_ratio if penalty is 'elasticnet'\n",
    "            if params['penalty'] == 'elasticnet':\n",
    "                params['l1_ratio'] = trial.suggest_float('l1_ratio', 0, 1)\n",
    "\n",
    "            # Prune invalid combinations:\n",
    "            if (\n",
    "                (params['solver'] == 'lbfgs' and params['penalty'] not in ['l2', None]) or\n",
    "                (params['solver'] == 'liblinear' and params['penalty'] not in ['l1', 'l2']) or\n",
    "                (params['solver'] == 'sag' and params['penalty'] not in ['l2', None]) or\n",
    "                (params['solver'] == 'newton-cholesky' and params['penalty'] not in ['l2', None]) or\n",
    "                (params['solver'] == 'saga' and params['penalty'] not in ['elasticnet', 'l1', 'l2', None])\n",
    "            ):\n",
    "                raise optuna.TrialPruned()  # Invalid combination of solver and penalty\n",
    "\n",
    "            return params\n",
    "\n",
    "        \n",
    "        elif classifier_name == \"GradientBoosting\":\n",
    "            return {\n",
    "                \"learning_rate\" : trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "                \"n_estimators\" : trial.suggest_int('n_estimators', 100, 1000),\n",
    "                \"max_depth\" : trial.suggest_int('max_depth', 3, 10),\n",
    "                \"min_samples_split\" : trial.suggest_int('min_samples_split', 2, 20),\n",
    "                \"min_samples_leaf\" : trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "                \"max_features\" : trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    \n",
    "            }\n",
    "        elif classifier_name == \"KNeighbors\":\n",
    "            params = {\n",
    "                \"n_neighbors\": trial.suggest_int('n_neighbors', 1, 50),\n",
    "                \"weights\": trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "                \"p\": trial.suggest_int('p', 1, 2),  # 1: Manhattan, 2: Euclidean\n",
    "                \"leaf_size\": trial.suggest_int('leaf_size', 10, 100),\n",
    "                \"metric\": trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski', 'chebyshev'])\n",
    "            }\n",
    "            return params\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid classifier name: {classifier_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFactory:\n",
    "    \"\"\"\n",
    "    A class to create model instances with additional parameters for specific classifiers.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): The name of the model to be instantiated.\n",
    "        best_params (dict): The best hyperparameters for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, best_params: dict):\n",
    "        \"\"\"\n",
    "        Initialize the ModelFactory with a model name and parameters.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The name of the model.\n",
    "            best_params (dict): Hyperparameters for the model.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.best_params = best_params\n",
    "\n",
    "    def get_model_instance(self):\n",
    "        \"\"\"\n",
    "        Creates a model instance based on the model name with additional classifier-specific parameters.\n",
    "\n",
    "        Returns:\n",
    "            A model instance with the appropriate parameters.\n",
    "        \"\"\"\n",
    "        # Dictionary of model classes\n",
    "        model_dict = {\n",
    "            \"LGBM\": LGBMClassifier,\n",
    "            \"XGBoost\": XGBClassifier,\n",
    "            \"CatBoost\": CatBoostClassifier,\n",
    "            \"RandomForest\": RandomForestClassifier,\n",
    "            \"DecisionTree\": DecisionTreeClassifier,\n",
    "            \"LogisticRegression\": LogisticRegression,\n",
    "            \"SVC\": SVC,\n",
    "            \"GradientBoosting\": GradientBoostingClassifier,\n",
    "            \"KNeighbors\": KNeighborsClassifier\n",
    "        }\n",
    "\n",
    "        # Check if the model exists in the model_dict\n",
    "        if self.model_name not in model_dict:\n",
    "            raise ValueError(f\"Model {self.model_name} is not supported.\")\n",
    "\n",
    "        # Create a model instance with specific parameters\n",
    "        if self.model_name == \"LGBM\":\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42, verbose=-1)  # Add verbose for LGBM\n",
    "        elif self.model_name == \"RandomForest\":\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42, n_jobs=-1)  # Add n_jobs for RandomForest\n",
    "        elif self.model_name == \"SVC\":\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42, probability=True)  # Add probability for SVC\n",
    "        elif self.model_name == \"CatBoost\":\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42, verbose=0)  # Suppress CatBoost verbosity\n",
    "        elif self.model_name == \"KNeighbors\":\n",
    "            return model_dict[self.model_name](**self.best_params)  # Suppress CatBoost verbosity\n",
    "        else:\n",
    "            return model_dict[self.model_name](**self.best_params, random_state=42)  # Default for other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineManager:\n",
    "    \"\"\"\n",
    "    A class that handles both building and modifying pipelines dynamically.\n",
    "    This class supports both scikit-learn's Pipeline and imbalanced-learn's Pipeline.\n",
    "\n",
    "    It allows the construction of the initial pipeline and the insertion of steps \n",
    "    at any position within the pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pipeline_type='ImbPipeline'):\n",
    "        \"\"\"\n",
    "        Initialize the PipelineManager with a specified pipeline type.\n",
    "\n",
    "        Args:\n",
    "            pipeline_type (str): The type of pipeline to use ('ImbPipeline' or 'Pipeline').\n",
    "        \"\"\"\n",
    "        if pipeline_type == 'ImbPipeline':\n",
    "            self.pipeline = ImbPipeline(steps=[])\n",
    "        elif pipeline_type == 'Pipeline':\n",
    "            self.pipeline = Pipeline(steps=[])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pipeline type. Choose 'ImbPipeline' or 'Pipeline'.\")\n",
    "\n",
    "    def add_step(self, step_name, step_object, position=None):\n",
    "        \"\"\"\n",
    "        Add a transformation step to the pipeline.\n",
    "\n",
    "        Args:\n",
    "            step_name (str): Name of the step to add.\n",
    "            step_object (object): The transformer or estimator object (e.g., scaler, classifier).\n",
    "            position (int or None): Optional; the position to insert the step.\n",
    "                                    If None, the step is appended at the end of the pipeline.\n",
    "        \"\"\"\n",
    "        if position is None:\n",
    "            self.pipeline.steps.append((step_name, step_object))\n",
    "        else:\n",
    "            self.pipeline.steps.insert(position, (step_name, step_object))\n",
    "\n",
    "    def remove_step(self, step_name):\n",
    "        \"\"\"\n",
    "        Remove a step from the pipeline by its name.\n",
    "\n",
    "        Args:\n",
    "            step_name (str): The name of the step to remove.\n",
    "        \"\"\"\n",
    "        self.pipeline.steps = [(name, step) for name, step in self.pipeline.steps if name != step_name]\n",
    "\n",
    "    def replace_step(self, step_name, new_step_object):\n",
    "        \"\"\"\n",
    "        Replace an existing step in the pipeline with a new step.\n",
    "\n",
    "        Args:\n",
    "            step_name (str): The name of the step to replace.\n",
    "            new_step_object (object): The new transformer or estimator object.\n",
    "        \"\"\"\n",
    "        for i, (name, step) in enumerate(self.pipeline.steps):\n",
    "            if name == step_name:\n",
    "                self.pipeline.steps[i] = (step_name, new_step_object)\n",
    "                break\n",
    "\n",
    "    def get_pipeline(self):\n",
    "        \"\"\"\n",
    "        Get the constructed or modified pipeline.\n",
    "\n",
    "        Returns:\n",
    "            Pipeline: The constructed or modified pipeline object.\n",
    "        \"\"\"\n",
    "        return self.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "    \"\"\"\n",
    "    A class that encapsulates the preprocessing steps for feature engineering,\n",
    "    imputation, scaling, encoding, and transformations. This can be inserted into\n",
    "    the overall pipeline before the model fitting step.\n",
    "    \"\"\"\n",
    "    def __init__(self, bins_hour, names_period, drop_columns, numerical_features,\n",
    "                 onehot_features, ordinal_features, transform_features, trial: optuna.Trial=None):\n",
    "        \"\"\"\n",
    "        Initialize the PreprocessingPipeline with necessary parameters.\n",
    "\n",
    "        Args:\n",
    "            bins_hour: Parameters for creating new features from hourly bins.\n",
    "            names_period: Period names for feature creation.\n",
    "            drop_columns: Columns to be dropped from the dataset.\n",
    "            numerical_features: List of numerical features for processing.\n",
    "            onehot_features: List of categorical features for OneHot encoding.\n",
    "            ordinal_features: List of ordinal features for Ordinal encoding.\n",
    "            transform_features: Features that require power transformation.\n",
    "        \"\"\"\n",
    "        self.bins_hour = bins_hour\n",
    "        self.names_period = names_period\n",
    "        self.drop_columns = drop_columns\n",
    "        self.numerical_features = numerical_features\n",
    "        self.onehot_features = onehot_features\n",
    "        self.ordinal_features = ordinal_features\n",
    "        self.transform_features = transform_features\n",
    "        \n",
    "    def instantiate_numerical_simple_imputer(self, trial: optuna.Trial=None, strategy: str=None, fill_value: int=-1) -> SimpleImputer:\n",
    "        if strategy is None and trial:\n",
    "            strategy = trial.suggest_categorical(\n",
    "                'numerical_strategy', ['mean', 'median', 'most_frequent']\n",
    "            )\n",
    "        #print(f\"instantiate_numerical_simple_imputer: strategy= {strategy}\")\n",
    "        return SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "\n",
    "    def instantiate_categorical_simple_imputer(self, trial: optuna.Trial=None, strategy: str=None, fill_value: str='missing') -> SimpleImputer:\n",
    "        if strategy is None and trial:\n",
    "            strategy = trial.suggest_categorical('categorical_strategy', ['most_frequent', 'constant'])\n",
    "        #print(f\"instantiate_categorical_simple_imputer: strategy= {strategy}\")\n",
    "        return SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "    \n",
    "    def instantiate_outliers(self, trial: optuna.Trial=None, strategy=None) -> Union[PowerTransformer, FunctionTransformer, OutlierDetector]:\n",
    "        \"\"\"\n",
    "        Instantiate outlier handling method: PowerTransformer, LogTransformer, or OutlierDetector.\n",
    "\n",
    "        Args:\n",
    "            trial (optuna.Trial, optional): The trial object for hyperparameter optimization.\n",
    "\n",
    "        Returns:\n",
    "            Union[PowerTransformer, FunctionTransformer, OutlierDetector]: The selected outlier handling method.\n",
    "        \"\"\"\n",
    "        # Suggest from available options\n",
    "        options = ['power_transform', 'log_transform', 'iqr_clip', 'iqr_median', 'iqr_mean']\n",
    "        if trial:\n",
    "            strategy = trial.suggest_categorical('outlier_strategy', options)\n",
    "        else:\n",
    "            strategy = strategy  # Default to first option if no trial is provided\n",
    "\n",
    "        if strategy == 'power_transform':\n",
    "            return PowerTransformer(method='yeo-johnson')\n",
    "        elif strategy == 'log_transform':\n",
    "            return LogTransformer()\n",
    "            #return FunctionTransformer(np.log1p)  # Log transformation\n",
    "        elif strategy in ['iqr_clip', 'iqr_median', 'iqr_mean']:\n",
    "            return OutlierHandler(strategy=strategy)  # Instantiate OutlierDetector\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy for outlier handling: {strategy}\")\n",
    "\n",
    "         \n",
    "    def build(self, step_name=None, trial: optuna.Trial=None, **column_transformer_strategy):\n",
    "        \"\"\"\n",
    "        Build the preprocessing pipeline with feature creation, transformation, \n",
    "        imputation, scaling, and encoding steps.\n",
    "        \n",
    "        Returns:\n",
    "            Transformer: The appropriate transformer for the given step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if step_name == \"create_new_features\":\n",
    "            return CreateNewFeature(bins_hour=self.bins_hour, names_period=self.names_period)\n",
    "        \n",
    "        if step_name == \"replace_class\":\n",
    "            return ReplaceValueTransformer(old_value=\"?\", new_value=np.nan)\n",
    "        \n",
    "        if step_name == \"drop_cols\":\n",
    "            return DropRedundantColumns(redundant_cols=self.drop_columns)\n",
    "        \n",
    "        if step_name == 'column_transformer':\n",
    "            \n",
    "            numerical_strategy = column_transformer_strategy.get('numerical_strategy', None)\n",
    "            categorical_strategy = column_transformer_strategy.get('categorical_strategy',None)\n",
    "            outlier_strategy = column_transformer_strategy.get('outlier_strategy', None)\n",
    "        \n",
    "            \n",
    "            return ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('categorical', Pipeline([\n",
    "                        ('imputer', self.instantiate_categorical_simple_imputer(trial=trial, strategy=categorical_strategy)),   \n",
    "                        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))\n",
    "                    ]), self.onehot_features),\n",
    "                    \n",
    "                    ('numerical', Pipeline([\n",
    "                        ('imputer', self.instantiate_numerical_simple_imputer(trial=trial, strategy=numerical_strategy)),\n",
    "                        #('scaler', StandardScaler())  # Add scaler if needed\n",
    "                    ]), self.numerical_features),\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    ('ordinal', Pipeline([\n",
    "                        ('imputer', self.instantiate_categorical_simple_imputer(trial=trial, strategy=categorical_strategy)),\n",
    "                        ('ordinal', OrdinalEncoder())\n",
    "                    ]), self.ordinal_features),\n",
    "                    \n",
    "                    ('outlier_transform', Pipeline([\n",
    "                        ('imputer', self.instantiate_numerical_simple_imputer(trial=trial, strategy=numerical_strategy)),\n",
    "                        ('outlier_transformer', self.instantiate_outliers(trial=trial, strategy=outlier_strategy))  # Update this line\n",
    "                    ]), self.transform_features),\n",
    "                ],\n",
    "                remainder='passthrough'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResamplerSelector:\n",
    "    \"\"\"\n",
    "    A class to select and return a resampling algorithm based on a given parameter or \n",
    "    from a trial suggestion if available.\n",
    "\n",
    "    Attributes:\n",
    "        trial (optuna.trial, optional): The trial object for hyperparameter optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trial=None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the ResamplerSelector with an optional trial for hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            trial (optuna.trial, optional): An optional trial object for suggesting resampling strategies.\n",
    "            random_state (int): Random seed for reproducibility. Default is 42.\n",
    "        \"\"\"\n",
    "        self.trial = trial\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_resampler(self, resampler=None):\n",
    "        \"\"\"\n",
    "        Return the resampling algorithm based on the provided `resampler` parameter.\n",
    "        If `resampler` is not given, it is suggested from the trial.\n",
    "\n",
    "        Args:\n",
    "            resampler (str, optional): The resampling method ('RandomOverSampler', 'ADASYN', etc.). \n",
    "                                       If not provided, it will be suggested from the trial (if available).\n",
    "\n",
    "        Returns:\n",
    "            resampler_obj (object): The resampling instance based on the selected method.\n",
    "        \"\"\"\n",
    "        if resampler is None and self.trial:\n",
    "            resampler = self.trial.suggest_categorical(\n",
    "                'resampler', ['RandomOverSampler', 'SMOTEENN', 'SMOTETomek']\n",
    "            )\n",
    "            #['RandomOverSampler', 'ADASYN', 'RandomUnderSampler', 'NearMiss', 'SMOTEENN', 'SMOTETomek']\n",
    "\n",
    "        if resampler == 'RandomOverSampler':\n",
    "            return RandomOverSampler(random_state=self.random_state)\n",
    "        elif resampler == 'ADASYN':\n",
    "            return ADASYN(random_state=self.random_state)\n",
    "        elif resampler == 'RandomUnderSampler':\n",
    "            return RandomUnderSampler(random_state=self.random_state)\n",
    "        elif resampler == 'NearMiss':\n",
    "            return NearMiss()\n",
    "        elif resampler == 'SMOTEENN':\n",
    "            return SMOTEENN(random_state=self.random_state, sampling_strategy='minority' )\n",
    "        elif resampler == 'SMOTETomek':\n",
    "            return SMOTETomek(random_state=self.random_state)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown resampler: {resampler}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalerSelector:\n",
    "    \"\"\"\n",
    "    A class to select and return a scaling algorithm based on a given parameter or \n",
    "    from a trial suggestion if available.\n",
    "\n",
    "    Attributes:\n",
    "        trial (optuna.trial, optional): The trial object for hyperparameter optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trial=None):\n",
    "        \"\"\"\n",
    "        Initialize the ScalerSelector with an optional trial for hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            trial (optuna.trial, optional): An optional trial object for suggesting resampling strategies.\n",
    "        \"\"\"\n",
    "        self.trial = trial\n",
    "\n",
    "    def get_scaler(self, scaler_name=None):\n",
    "        \"\"\"\n",
    "        Return the scaling algorithm based on the provided `scaler_name` parameter.\n",
    "        If `scaler_name` is not given, it is suggested from the trial.\n",
    "\n",
    "        Args:\n",
    "            scaler_name (str, optional): The scalring method ('MinMaxScaler', 'StandardScaler', etc.). \n",
    "                                       If not provided, it will be suggested from the trial (if available).\n",
    "\n",
    "        Returns:\n",
    "            rscaler_obj (object): The scaling instance based on the selected method.\n",
    "        \"\"\" \n",
    "         \n",
    "        # -- Instantiate scaler (skip scaler for CatBoost as it handles categorical features internally)\n",
    "        if scaler_name is None and self.trial:\n",
    "            scaler_name = self.trial.suggest_categorical(\"scaler\", ['minmax', 'standard', 'robust'])\n",
    "            \n",
    "        if scaler_name == \"minmax\":\n",
    "            return MinMaxScaler()\n",
    "        elif scaler_name == \"standard\":\n",
    "            return StandardScaler()\n",
    "        elif scaler_name == \"robust\":\n",
    "            return RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scaler: {scaler_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionalityReductionSelector:\n",
    "    \"\"\"\n",
    "    A class to select and return a dimensionality reduction algorithm based on a given parameter \n",
    "    or from a trial suggestion if available.\n",
    "\n",
    "    Attributes:\n",
    "        trial (optuna.trial, optional): The trial object for hyperparameter optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trial=None):\n",
    "        \"\"\"\n",
    "        Initialize the DimensionalityReductionSelector with an optional trial for hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            trial (optuna.trial, optional): An optional trial object for suggesting dimensionality reduction strategies.\n",
    "        \"\"\"\n",
    "        self.trial = trial\n",
    "\n",
    "    def get_dimensionality_reduction(self, dim_red=None, pca_n_components=5):\n",
    "        \"\"\"\n",
    "        Return the dimensionality reduction algorithm based on the provided `dim_red` parameter.\n",
    "        If `dim_red` is not given, it is suggested from the trial.\n",
    "\n",
    "        Args:\n",
    "            dim_red (str, optional): The dimensionality reduction method ('PCA' or None). If not provided,\n",
    "                                     it will be suggested from the trial (if available).\n",
    "\n",
    "        Returns:\n",
    "            dimen_red_algorithm (object or str): PCA algorithm or 'passthrough'.\n",
    "        \"\"\"\n",
    "        if dim_red is None and self.trial:\n",
    "            dim_red = self.trial.suggest_categorical(\"dim_red\", [\"PCA\", None])\n",
    "\n",
    "        if dim_red == \"PCA\":\n",
    "            if self.trial:\n",
    "                pca_n_components = self.trial.suggest_int(\"pca_n_components\", 2, 30)\n",
    "            else:\n",
    "                pca_n_components = pca_n_components  # Default value if trial is not provided\n",
    "            dimen_red_algorithm = PCA(n_components=pca_n_components)\n",
    "        else:\n",
    "            dimen_red_algorithm = 'passthrough'\n",
    "\n",
    "        return dimen_red_algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_model_and_params(classifier_name, trial, model_mode='tune', input_params=None):\n",
    "    # Got the Preprocessed Pipeline containting Data Cleaning and Column Transformation\n",
    "    \n",
    "                    \n",
    "    preprocessing_pipeline = PreprocessingPipeline(\n",
    "        bins_hour=bins_hour,\n",
    "        names_period=names_period,\n",
    "        drop_columns=drop_columns,\n",
    "        numerical_features=numerical_features,\n",
    "        onehot_features=onehot_features,\n",
    "        ordinal_features=ordinal_features,\n",
    "        transform_features=transform_features\n",
    "    )\n",
    "\n",
    "\n",
    "    #print(f\"get_pipeline_model_and_params: Starting input params: {input_params}\")\n",
    "\n",
    "    # Initialize the manager with the preferred pipeline type ('ImbPipeline' or 'Pipeline')\n",
    "    pipeline_manager = PipelineManager(pipeline_type='ImbPipeline')\n",
    "    \n",
    "    pipeline_manager.add_step('create_new_features', preprocessing_pipeline.build(step_name='create_new_features', trial=None), position=0)\n",
    "    pipeline_manager.add_step('replace_class', preprocessing_pipeline.build(step_name='replace_class', trial=None), position=1)\n",
    "    pipeline_manager.add_step('drop_cols', preprocessing_pipeline.build(step_name='drop_cols', trial=None), position=2)\n",
    "\n",
    "    \n",
    "    if model_mode == 'tune':\n",
    "        # Add transformation steps: Option 3       \n",
    "        pipeline_manager.add_step('column_transformer', preprocessing_pipeline.build(step_name='column_transformer', trial=trial), position=3)\n",
    "    else:\n",
    "        # Add transformation steps: Option 3 = Works perfectly\n",
    "        numerical_strategy = input_params.pop('numerical_strategy', 'mean')\n",
    "        categorical_strategy = input_params.pop('categorical_strategy','most_frequent')\n",
    "        outlier_strategy = input_params.pop('outlier_strategy', 'power_transform')\n",
    "        \n",
    "        print(f\"numerical_strategy: {numerical_strategy}\")\n",
    "        print(f\"categorical_strategy: {categorical_strategy}\")\n",
    "        print(f\"outlier_strategy: {outlier_strategy}\")\n",
    "        \n",
    "        \n",
    "        column_transformer_strategy ={\n",
    "            \"numerical_strategy\": numerical_strategy,\n",
    "            \"categorical_strategy\": categorical_strategy,\n",
    "            \"outlier_strategy\": outlier_strategy,\n",
    "            \"missing_values\": 'mean',\n",
    "            \"handle_unknown\": 'ignore'  # It's important to handle unknown categorical values correctly when doing feature engineering.\n",
    "        }\n",
    "        \n",
    "        pipeline_manager.add_step('column_transformer', preprocessing_pipeline.build(step_name='column_transformer', trial=None, **column_transformer_strategy), position=3)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Add the resampler step based on the provided resample name or trial suggestion\n",
    "    resample_selector = ResamplerSelector(trial=trial) \n",
    "    resampler = input_params.pop('resampler', None)   \n",
    "    resampler_obj = resample_selector.get_resampler(resampler=resampler)\n",
    "    pipeline_manager.add_step('resampler', resampler_obj, position=4)\n",
    "    \n",
    "    \n",
    "    # Add the scaler step based on the provided resample name or trial suggestion\n",
    "    scaler_selector = ScalerSelector(trial=trial)\n",
    "    scaler_name = input_params.pop('scaler', None)      \n",
    "    scaler_obj = scaler_selector.get_scaler(scaler_name=scaler_name)\n",
    "    pipeline_manager.add_step('scaler', scaler_obj, position=5)\n",
    "    \n",
    "    \n",
    "    # Add the Dimensional Reduction step based on the provided parameter or trial suggestion\n",
    "    dim_red_selector = DimensionalityReductionSelector(trial=trial)\n",
    "    dim_red = input_params.pop('dim_red', None)\n",
    "    pca_n_components = input_params.get('pca_n_components', 5)  \n",
    "    dim_red_obj = dim_red_selector.get_dimensionality_reduction(dim_red=dim_red, pca_n_components=pca_n_components)\n",
    "    pipeline_manager.add_step('dim_reduction', dim_red_obj, position=6)\n",
    "\n",
    "    # Create an instance of the ModelFactory class with best_model and best_params\n",
    "    #print(f\"Input params into the classifer:\\n{input_params}\")\n",
    "    model_factory = ModelFactory(classifier_name, input_params)\n",
    "    model_obj = model_factory.get_model_instance()\n",
    "    pipeline_manager.add_step('model', model_obj, position=7)\n",
    "    \n",
    "    pipeline = pipeline_manager.get_pipeline()\n",
    "    \n",
    "    return pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function for Optuna\n",
    "def objective(classifier_name: str, trial: optuna.Trial=None, scoring='f1') -> float:\n",
    "    \"\"\"\n",
    "    Objective function to optimize classifiers dynamically using Optuna.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object for suggesting hyperparameters.\n",
    "        classifier_name (str): Classifier to optimize.\n",
    "        scoring (str): Scoring metric for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean score from cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get hyperparameters for the classifier from HyperparameterTuner\n",
    "    hyperparameter_tuner = HyperparameterTuner()\n",
    "    params = hyperparameter_tuner.get_params(trial, classifier_name)\n",
    "    #print(\"hyperparameter parameters obtained from HyperparameterTuner class\")\n",
    "    \n",
    "    # Got the Preprocessed Pipeline containting Data Cleaning and Column Transformation\n",
    "    \n",
    "    pipeline = get_pipeline_model_and_params(classifier_name, trial, model_mode='tune', input_params=params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    score = cross_val_score(pipeline, X_train, y_train, scoring=scoring, n_jobs=-1, cv=kfold, verbose=0, error_score='raise')\n",
    "    score_training = score.mean()\n",
    "    \n",
    "    #pipeline.fit(X_train, y_train)\n",
    "    #score_testing = pipeline.score(X_test, y_test)\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        \n",
    "    tuning_test_metrics = {\n",
    "    'f1': f1,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'roc_auc': roc_auc,\n",
    "    'training_score': score_training,\n",
    "\n",
    "}\n",
    "    \n",
    "    return tuning_test_metrics[scoring]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another objective function that shares most of the `objective` function to reproduce the model with the best hyperparameters.\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def detailed_objective(classifier_name: str, trial: optuna.Trial=None, scoring='f1') -> Tuple[float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Objective function to optimize classifiers dynamically using Optuna.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object for suggesting hyperparameters.\n",
    "        classifier_name (str): Classifier to optimize.\n",
    "        scoring (str): Scoring metric for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean score from cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get hyperparameters for the classifier from HyperparameterTuner\n",
    "    hyperparameter_tuner = HyperparameterTuner()\n",
    "    params = hyperparameter_tuner.get_params(trial, classifier_name)\n",
    "    #print(\"hyperparameter parameters obtained from HyperparameterTuner class\")\n",
    "    \n",
    "    # Got the Preprocessed Pipeline containting Data Cleaning and Column Transformation\n",
    "    \n",
    "    pipeline = get_pipeline_model_and_params(classifier_name, trial, model_mode='tune', input_params=params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    score = cross_val_score(pipeline, X_train, y_train, scoring=scoring, n_jobs=-1, cv=kfold, verbose=0, error_score='raise')\n",
    "    score_training = score.mean()\n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    metrics_test = {\n",
    "        'classifier_name': classifier_name,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc,\n",
    "        f'score_training_{scoring}': score_training,\n",
    "        'best_params': trial,\n",
    "        \"classification_report\": classification_report(y_test, y_pred, output_dict=True)  # Detailed report\n",
    "        }\n",
    "    \n",
    "    # Save the variables to a file\n",
    "    # Serialise the trained pipeline\n",
    "    joblib.dump((pipeline, metrics_test), f'{classifier_name}_pipeline.pkl')\n",
    "    print(f'Serialized {classifier_name} pipeline and test metrics to {classifier_name}_pipeline.pkl')\n",
    "    \n",
    "     \n",
    "\n",
    "    return metrics_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Optuna study\n",
    "def run_optimization(config_path: str, n_trials: int = 100, scoring: str = 'f1') -> None:\n",
    "    \"\"\"\n",
    "    Run Optuna study for hyperparameter tuning and model selection.\n",
    "    \n",
    "    Args:\n",
    "        config_path (str): Path to the YAML configuration file.\n",
    "        n_trials (int): Number of trials for optimization. Defaults to 100.\n",
    "        scoring (str): Scoring metric for optimization. Defaults to 'f1'.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_model_score = 0\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    best_of_models = []\n",
    "\n",
    "    all_models = [\"RandomForest\", \"DecisionTree\", \n",
    "                  \"XGBoost\", \"LGBM\", \"GradientBoosting\", \n",
    "                  \"LogisticRegression\", \"KNeighbors\", \"CatBoost\"]\n",
    "    \n",
    "    #all_models = [\"DecisionTree\", \"RandomForest\", \"LogisticRegression\"]\n",
    "\n",
    "    for classifier_name in all_models:\n",
    "        print(f\"Optimizing model: {classifier_name} | Scoring: {scoring}\")\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=11))\n",
    "        study.optimize(lambda trial: objective(classifier_name, trial, scoring), n_trials=n_trials)\n",
    "        \n",
    "        print(f\"classifier_name: {classifier_name} | study.best_trial.value: {study.best_trial.value}\")\n",
    "\n",
    "        best_trial = study.best_trial\n",
    "        best_of_models.append({\n",
    "            \"model\": classifier_name,\n",
    "            \"model_score_params\": best_trial.params,\n",
    "            \"model_score_trial_number\": best_trial.number,\n",
    "            \"model_score_datetime\": best_trial.datetime_start,\n",
    "            \"model_score_duration\": best_trial.duration,\n",
    "            \"model_score_status\": best_trial.state,\n",
    "            \"model_score_key\": scoring,\n",
    "            \"model_score_value\": best_trial.value\n",
    "            \n",
    "        })\n",
    "\n",
    "        current_score = best_trial.value\n",
    "        \n",
    "        \n",
    "        if current_score and current_score > best_model_score:\n",
    "            best_model_score = best_trial.value\n",
    "            best_model = classifier_name\n",
    "            best_params = best_trial.params\n",
    "            \n",
    "        print(f\"Current Model: {classifier_name}, Current Score: {current_score} | Best Model: {best_model}, Best Score: {best_model_score}\")\n",
    "            \n",
    "        best_parameters_results = detailed_objective(classifier_name=classifier_name, trial=study.best_trial,  scoring=scoring)\n",
    "        print(f\"Best Parameters: {best_parameters_results}\")\n",
    "        \n",
    "\n",
    "    # Display all results and the best model\n",
    "    for result in best_of_models:\n",
    "        print(result)\n",
    "    \n",
    "    print(\"Best model:\", best_model)\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    \n",
    "    # Save the variables to a file\n",
    "    with open(\"best_model_and_params.pkl\", \"wb\") as f:\n",
    "        joblib.dump((best_model, best_params), f)\n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "    return best_model, best_params, current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-11-05 09:31:58,483] Trial 11 failed with parameters: {'objective': 'CrossEntropy', 'colsample_bylevel': 0.07878989475519685, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'categorical_strategy': 'constant', 'numerical_strategy': 'median', 'outlier_strategy': 'iqr_mean', 'resampler': 'SMOTEENN', 'scaler': 'robust', 'dim_red': None} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/xh/sd37kh3d117gtcym3lygy3000000gn/T/ipykernel_10123/3688297276.py\", line 26, in <lambda>\n",
      "    study.optimize(lambda trial: objective(classifier_name, trial, scoring), n_trials=n_trials)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/xh/sd37kh3d117gtcym3lygy3000000gn/T/ipykernel_10123/2236160466.py\", line 26, in objective\n",
      "    score = cross_val_score(pipeline, X_train, y_train, scoring=scoring, n_jobs=-1, cv=kfold, verbose=0, error_score='raise')\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 712, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 423, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 74, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/joblib/parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/joblib/parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"/Users/donadviser/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/joblib/parallel.py\", line 1762, in _retrieve\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-05 09:31:58,489] Trial 11 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     best_model, best_params, current_score \u001b[38;5;241m=\u001b[39m \u001b[43mrun_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mrun_optimization\u001b[0;34m(config_path, n_trials, scoring)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizing model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Scoring: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscoring\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m))\n\u001b[0;32m---> 26\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier_name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | study.best_trial.value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mrun_optimization.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizing model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Scoring: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscoring\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m))\n\u001b[0;32m---> 26\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39mn_trials)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier_name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | study.best_trial.value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(classifier_name, trial, scoring)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Cross-validation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m kfold \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraise\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m score_training \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#pipeline.fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#score_testing = pipeline.score(X_test, y_test)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/venv311_insure/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config_path = \"model_config.yaml\"\n",
    "    best_model, best_params, current_score = run_optimization(config_path, n_trials=30, scoring='f1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to deserialise a pipeline for inference\n",
    "def load_pipeline(model_name):\n",
    "    return joblib.load(f'{model_name}_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the serialized model and metrics test results\n",
    "pipeline, metrics_test = load_pipeline(\"LogisticRegression\")\n",
    "# Perform predictions\n",
    "#predictions = pipeline.predict(new_data)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables from the file\n",
    "with open(\"fitted_best_model_and_params.pkl\", \"rb\") as f:\n",
    "    model_name, pipeline_fitted = joblib.load(f)\n",
    "\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"pipeline_fitted: {pipeline_fitted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fitted.fit(X_train, y_train)\n",
    "y_pred = pipeline_fitted.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "y_pred_proba = pipeline_fitted.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba) #Calculate Roc\n",
    "\n",
    "test_results = {\n",
    "    'f1_score': f1,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'roc_auc': roc_auc,\n",
    "    'model_name': model_name,\n",
    "    'best_params': best_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best_model: {best_model}\")\n",
    "print(f\"best_params: {best_params}\")\n",
    "\n",
    "# Create an instance of the ModelFactory class with best_model and best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables from the file\n",
    "with open(\"best_model_and_params.pkl\", \"rb\") as f:\n",
    "    best_model, best_params = joblib.load(f)\n",
    "\n",
    "print(f\"best_model: {best_model}\")\n",
    "print(f\"best_params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_pipeline_model_and_params(best_model, trial=None, model_mode='train', input_params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba) #Calculate Roc\n",
    "\n",
    "test_results = {\n",
    "    'f1_score': f1,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'roc_auc': roc_auc,\n",
    "    'best_model': best_model,\n",
    "    'best_params': best_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `pipeline` is your final fitted pipeline\n",
    "# Also, assume `X_train` is your training DataFrame\n",
    "\n",
    "# Fit the pipeline (if not already fitted)\n",
    "#pipeline.fit(X_train)\n",
    "\n",
    "# If you have a ColumnTransformer, find it in the pipeline\n",
    "column_transformer = None\n",
    "for name, step in pipeline.named_steps.items():\n",
    "    if isinstance(step, ColumnTransformer):\n",
    "        column_transformer = step\n",
    "        break\n",
    "\n",
    "# If you have a ColumnTransformer, get the feature names\n",
    "if column_transformer is not None:\n",
    "    # Get transformed column names\n",
    "    feature_names = []\n",
    "    for name, transformer, columns in column_transformer.transformers_:\n",
    "        if transformer != 'drop':\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                feature_names.extend(transformer.get_feature_names_out(columns))\n",
    "            else:\n",
    "                # If the transformer does not have get_feature_names_out method\n",
    "                feature_names.extend(columns)\n",
    "else:\n",
    "    # No ColumnTransformer, fallback to input features\n",
    "    feature_names = X_train.columns.tolist()\n",
    "\n",
    "print(\"Extracted Feature Names: \", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_feature_importance(model, X, y, feature_names, n_top=10):\n",
    "    \"\"\"\n",
    "    This function takes in a dictionary of models, the dataset X, y, and the feature names.\n",
    "    It fits each model, extracts feature importances (if available), \n",
    "    and plots the top n features.\n",
    "\n",
    "    Parameters:\n",
    "    models (dict): A dictionary containing model names and their respective model objects.\n",
    "    X (np.ndarray): Feature dataset.\n",
    "    y (pd.Series): Target variable.\n",
    "    feature_names (list): List of feature names after transformations.\n",
    "    n_top (int): Number of top features to display. Default is 10.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    model_name = 'classifier'\n",
    "     \n",
    "    # Fit the model\n",
    "    #model.fit(X, y)\n",
    "    print(f\"Feature ranking for model: {model_name}\")\n",
    "    try:\n",
    "        # Check if the model has the attribute `feature_importances_`\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # Get feature importances\n",
    "            importance_scores = model.feature_importances_\n",
    "            \n",
    "        else:                 \n",
    "            print(f\"{model_name} does not support feature importances.\")\n",
    "            importance_scores = model.coef_[0]\n",
    "            \n",
    "        # Create a DataFrame from feature names and importances\n",
    "        data = {'Feature': feature_names, 'Score': importance_scores}\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        \n",
    "        # Take the absolute value of the score\n",
    "        df['Abs_Score'] = np.abs(df['Score'])\n",
    "        \n",
    "        df_sorted = df.sort_values(by=\"Abs_Score\", ascending=False)\n",
    "        if n_top:\n",
    "            # Sort by absolute value of score in descending order (top 10)\n",
    "            df_sorted = df_sorted.head(n_top)\n",
    "        \n",
    "        # Define a color palette based on score values (positive = green, negative = red)\n",
    "        colors = [\"green\" if score > 0 else \"red\" for score in df_sorted[\"Score\"]]\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Create the bar chart with Seaborn\n",
    "        sns.barplot(x=\"Feature\", y=\"Score\", hue=\"Feature\", legend=False, data=df_sorted, palette=colors)\n",
    "        \n",
    "        # Customize the plot for better visual appeal\n",
    "        plt.xlabel(\"Feature\")\n",
    "        plt.ylabel(\"Feature Importance Score\")\n",
    "        plt.title(f\"Feature Importance in {model_name} Classification\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
    "        plt.tight_layout()  # Adjust spacing between elements\n",
    "    \n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "                \n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while extracting feature importances: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = pipeline.named_steps['model']\n",
    "fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the display feature importance function after model evaluation\n",
    "display_feature_importance(fitted_model, X_train, y_train, feature_names, n_top=10)  # Specify how many top features to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_feature_importance(model, feature_names=None, top_n=20):\n",
    "    \"\"\"\n",
    "    Plot the feature importance of a fitted tree-based model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator\n",
    "        A fitted tree-based model with `feature_importances_` attribute (e.g., RandomForest, GradientBoosting).\n",
    "    \n",
    "    feature_names : list or None\n",
    "        List of feature names. If None, numerical indices are used as feature names.\n",
    "    \n",
    "    top_n : int, default=20\n",
    "        The number of top features to plot. If `None`, all features will be plotted.\n",
    "    \"\"\"\n",
    "    # Check if the model has feature_importances_ attribute\n",
    "    try:\n",
    "        if not hasattr(model, 'feature_importances_'):\n",
    "            importance_values = model.coef_[0]\n",
    "        else:\n",
    "            # Get feature importance values and sort them in descending order\n",
    "            importance_values = model.feature_importances_\n",
    "    except:\n",
    "        raise ValueError(\"The model does not have `feature_importances_` attribute.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create a DataFrame for better manipulation and sorting\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature {i}' for i in range(len(importance_values))]\n",
    "    \n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance_values})\n",
    "    \n",
    "    # Sort features by importance\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # If top_n is specified, select the top_n features\n",
    "    if top_n is not None and top_n < len(importance_df):\n",
    "        importance_df = importance_df.head(top_n)\n",
    "    \n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df, hue='Feature', palette='viridis', dodge=False, legend=False)\n",
    "    plt.title('Top Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = pipeline.named_steps['model']\n",
    "fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to plot the feature importance\n",
    "plot_feature_importance(fitted_model, feature_names=feature_names, top_n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311_insure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
